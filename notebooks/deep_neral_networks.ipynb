{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XWuqngaQY22g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "from torch.nn import init\n",
        "import torchinfo\n",
        "from typing import *\n",
        "from types import FunctionType\n",
        "from torch import Tensor\n",
        "import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from enum import Enum\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from dataclasses import dataclass\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SgOFFgW37qR"
      },
      "source": [
        "### Torch Vision Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "o6fLsSQkqyd_"
      },
      "outputs": [],
      "source": [
        "# @title ImageNet Categories\n",
        "_IMAGENET_CATEGORIES = [\n",
        "    \"tench\",\n",
        "    \"goldfish\",\n",
        "    \"great white shark\",\n",
        "    \"tiger shark\",\n",
        "    \"hammerhead\",\n",
        "    \"electric ray\",\n",
        "    \"stingray\",\n",
        "    \"cock\",\n",
        "    \"hen\",\n",
        "    \"ostrich\",\n",
        "    \"brambling\",\n",
        "    \"goldfinch\",\n",
        "    \"house finch\",\n",
        "    \"junco\",\n",
        "    \"indigo bunting\",\n",
        "    \"robin\",\n",
        "    \"bulbul\",\n",
        "    \"jay\",\n",
        "    \"magpie\",\n",
        "    \"chickadee\",\n",
        "    \"water ouzel\",\n",
        "    \"kite\",\n",
        "    \"bald eagle\",\n",
        "    \"vulture\",\n",
        "    \"great grey owl\",\n",
        "    \"European fire salamander\",\n",
        "    \"common newt\",\n",
        "    \"eft\",\n",
        "    \"spotted salamander\",\n",
        "    \"axolotl\",\n",
        "    \"bullfrog\",\n",
        "    \"tree frog\",\n",
        "    \"tailed frog\",\n",
        "    \"loggerhead\",\n",
        "    \"leatherback turtle\",\n",
        "    \"mud turtle\",\n",
        "    \"terrapin\",\n",
        "    \"box turtle\",\n",
        "    \"banded gecko\",\n",
        "    \"common iguana\",\n",
        "    \"American chameleon\",\n",
        "    \"whiptail\",\n",
        "    \"agama\",\n",
        "    \"frilled lizard\",\n",
        "    \"alligator lizard\",\n",
        "    \"Gila monster\",\n",
        "    \"green lizard\",\n",
        "    \"African chameleon\",\n",
        "    \"Komodo dragon\",\n",
        "    \"African crocodile\",\n",
        "    \"American alligator\",\n",
        "    \"triceratops\",\n",
        "    \"thunder snake\",\n",
        "    \"ringneck snake\",\n",
        "    \"hognose snake\",\n",
        "    \"green snake\",\n",
        "    \"king snake\",\n",
        "    \"garter snake\",\n",
        "    \"water snake\",\n",
        "    \"vine snake\",\n",
        "    \"night snake\",\n",
        "    \"boa constrictor\",\n",
        "    \"rock python\",\n",
        "    \"Indian cobra\",\n",
        "    \"green mamba\",\n",
        "    \"sea snake\",\n",
        "    \"horned viper\",\n",
        "    \"diamondback\",\n",
        "    \"sidewinder\",\n",
        "    \"trilobite\",\n",
        "    \"harvestman\",\n",
        "    \"scorpion\",\n",
        "    \"black and gold garden spider\",\n",
        "    \"barn spider\",\n",
        "    \"garden spider\",\n",
        "    \"black widow\",\n",
        "    \"tarantula\",\n",
        "    \"wolf spider\",\n",
        "    \"tick\",\n",
        "    \"centipede\",\n",
        "    \"black grouse\",\n",
        "    \"ptarmigan\",\n",
        "    \"ruffed grouse\",\n",
        "    \"prairie chicken\",\n",
        "    \"peacock\",\n",
        "    \"quail\",\n",
        "    \"partridge\",\n",
        "    \"African grey\",\n",
        "    \"macaw\",\n",
        "    \"sulphur-crested cockatoo\",\n",
        "    \"lorikeet\",\n",
        "    \"coucal\",\n",
        "    \"bee eater\",\n",
        "    \"hornbill\",\n",
        "    \"hummingbird\",\n",
        "    \"jacamar\",\n",
        "    \"toucan\",\n",
        "    \"drake\",\n",
        "    \"red-breasted merganser\",\n",
        "    \"goose\",\n",
        "    \"black swan\",\n",
        "    \"tusker\",\n",
        "    \"echidna\",\n",
        "    \"platypus\",\n",
        "    \"wallaby\",\n",
        "    \"koala\",\n",
        "    \"wombat\",\n",
        "    \"jellyfish\",\n",
        "    \"sea anemone\",\n",
        "    \"brain coral\",\n",
        "    \"flatworm\",\n",
        "    \"nematode\",\n",
        "    \"conch\",\n",
        "    \"snail\",\n",
        "    \"slug\",\n",
        "    \"sea slug\",\n",
        "    \"chiton\",\n",
        "    \"chambered nautilus\",\n",
        "    \"Dungeness crab\",\n",
        "    \"rock crab\",\n",
        "    \"fiddler crab\",\n",
        "    \"king crab\",\n",
        "    \"American lobster\",\n",
        "    \"spiny lobster\",\n",
        "    \"crayfish\",\n",
        "    \"hermit crab\",\n",
        "    \"isopod\",\n",
        "    \"white stork\",\n",
        "    \"black stork\",\n",
        "    \"spoonbill\",\n",
        "    \"flamingo\",\n",
        "    \"little blue heron\",\n",
        "    \"American egret\",\n",
        "    \"bittern\",\n",
        "    \"crane bird\",\n",
        "    \"limpkin\",\n",
        "    \"European gallinule\",\n",
        "    \"American coot\",\n",
        "    \"bustard\",\n",
        "    \"ruddy turnstone\",\n",
        "    \"red-backed sandpiper\",\n",
        "    \"redshank\",\n",
        "    \"dowitcher\",\n",
        "    \"oystercatcher\",\n",
        "    \"pelican\",\n",
        "    \"king penguin\",\n",
        "    \"albatross\",\n",
        "    \"grey whale\",\n",
        "    \"killer whale\",\n",
        "    \"dugong\",\n",
        "    \"sea lion\",\n",
        "    \"Chihuahua\",\n",
        "    \"Japanese spaniel\",\n",
        "    \"Maltese dog\",\n",
        "    \"Pekinese\",\n",
        "    \"Shih-Tzu\",\n",
        "    \"Blenheim spaniel\",\n",
        "    \"papillon\",\n",
        "    \"toy terrier\",\n",
        "    \"Rhodesian ridgeback\",\n",
        "    \"Afghan hound\",\n",
        "    \"basset\",\n",
        "    \"beagle\",\n",
        "    \"bloodhound\",\n",
        "    \"bluetick\",\n",
        "    \"black-and-tan coonhound\",\n",
        "    \"Walker hound\",\n",
        "    \"English foxhound\",\n",
        "    \"redbone\",\n",
        "    \"borzoi\",\n",
        "    \"Irish wolfhound\",\n",
        "    \"Italian greyhound\",\n",
        "    \"whippet\",\n",
        "    \"Ibizan hound\",\n",
        "    \"Norwegian elkhound\",\n",
        "    \"otterhound\",\n",
        "    \"Saluki\",\n",
        "    \"Scottish deerhound\",\n",
        "    \"Weimaraner\",\n",
        "    \"Staffordshire bullterrier\",\n",
        "    \"American Staffordshire terrier\",\n",
        "    \"Bedlington terrier\",\n",
        "    \"Border terrier\",\n",
        "    \"Kerry blue terrier\",\n",
        "    \"Irish terrier\",\n",
        "    \"Norfolk terrier\",\n",
        "    \"Norwich terrier\",\n",
        "    \"Yorkshire terrier\",\n",
        "    \"wire-haired fox terrier\",\n",
        "    \"Lakeland terrier\",\n",
        "    \"Sealyham terrier\",\n",
        "    \"Airedale\",\n",
        "    \"cairn\",\n",
        "    \"Australian terrier\",\n",
        "    \"Dandie Dinmont\",\n",
        "    \"Boston bull\",\n",
        "    \"miniature schnauzer\",\n",
        "    \"giant schnauzer\",\n",
        "    \"standard schnauzer\",\n",
        "    \"Scotch terrier\",\n",
        "    \"Tibetan terrier\",\n",
        "    \"silky terrier\",\n",
        "    \"soft-coated wheaten terrier\",\n",
        "    \"West Highland white terrier\",\n",
        "    \"Lhasa\",\n",
        "    \"flat-coated retriever\",\n",
        "    \"curly-coated retriever\",\n",
        "    \"golden retriever\",\n",
        "    \"Labrador retriever\",\n",
        "    \"Chesapeake Bay retriever\",\n",
        "    \"German short-haired pointer\",\n",
        "    \"vizsla\",\n",
        "    \"English setter\",\n",
        "    \"Irish setter\",\n",
        "    \"Gordon setter\",\n",
        "    \"Brittany spaniel\",\n",
        "    \"clumber\",\n",
        "    \"English springer\",\n",
        "    \"Welsh springer spaniel\",\n",
        "    \"cocker spaniel\",\n",
        "    \"Sussex spaniel\",\n",
        "    \"Irish water spaniel\",\n",
        "    \"kuvasz\",\n",
        "    \"schipperke\",\n",
        "    \"groenendael\",\n",
        "    \"malinois\",\n",
        "    \"briard\",\n",
        "    \"kelpie\",\n",
        "    \"komondor\",\n",
        "    \"Old English sheepdog\",\n",
        "    \"Shetland sheepdog\",\n",
        "    \"collie\",\n",
        "    \"Border collie\",\n",
        "    \"Bouvier des Flandres\",\n",
        "    \"Rottweiler\",\n",
        "    \"German shepherd\",\n",
        "    \"Doberman\",\n",
        "    \"miniature pinscher\",\n",
        "    \"Greater Swiss Mountain dog\",\n",
        "    \"Bernese mountain dog\",\n",
        "    \"Appenzeller\",\n",
        "    \"EntleBucher\",\n",
        "    \"boxer\",\n",
        "    \"bull mastiff\",\n",
        "    \"Tibetan mastiff\",\n",
        "    \"French bulldog\",\n",
        "    \"Great Dane\",\n",
        "    \"Saint Bernard\",\n",
        "    \"Eskimo dog\",\n",
        "    \"malamute\",\n",
        "    \"Siberian husky\",\n",
        "    \"dalmatian\",\n",
        "    \"affenpinscher\",\n",
        "    \"basenji\",\n",
        "    \"pug\",\n",
        "    \"Leonberg\",\n",
        "    \"Newfoundland\",\n",
        "    \"Great Pyrenees\",\n",
        "    \"Samoyed\",\n",
        "    \"Pomeranian\",\n",
        "    \"chow\",\n",
        "    \"keeshond\",\n",
        "    \"Brabancon griffon\",\n",
        "    \"Pembroke\",\n",
        "    \"Cardigan\",\n",
        "    \"toy poodle\",\n",
        "    \"miniature poodle\",\n",
        "    \"standard poodle\",\n",
        "    \"Mexican hairless\",\n",
        "    \"timber wolf\",\n",
        "    \"white wolf\",\n",
        "    \"red wolf\",\n",
        "    \"coyote\",\n",
        "    \"dingo\",\n",
        "    \"dhole\",\n",
        "    \"African hunting dog\",\n",
        "    \"hyena\",\n",
        "    \"red fox\",\n",
        "    \"kit fox\",\n",
        "    \"Arctic fox\",\n",
        "    \"grey fox\",\n",
        "    \"tabby\",\n",
        "    \"tiger cat\",\n",
        "    \"Persian cat\",\n",
        "    \"Siamese cat\",\n",
        "    \"Egyptian cat\",\n",
        "    \"cougar\",\n",
        "    \"lynx\",\n",
        "    \"leopard\",\n",
        "    \"snow leopard\",\n",
        "    \"jaguar\",\n",
        "    \"lion\",\n",
        "    \"tiger\",\n",
        "    \"cheetah\",\n",
        "    \"brown bear\",\n",
        "    \"American black bear\",\n",
        "    \"ice bear\",\n",
        "    \"sloth bear\",\n",
        "    \"mongoose\",\n",
        "    \"meerkat\",\n",
        "    \"tiger beetle\",\n",
        "    \"ladybug\",\n",
        "    \"ground beetle\",\n",
        "    \"long-horned beetle\",\n",
        "    \"leaf beetle\",\n",
        "    \"dung beetle\",\n",
        "    \"rhinoceros beetle\",\n",
        "    \"weevil\",\n",
        "    \"fly\",\n",
        "    \"bee\",\n",
        "    \"ant\",\n",
        "    \"grasshopper\",\n",
        "    \"cricket\",\n",
        "    \"walking stick\",\n",
        "    \"cockroach\",\n",
        "    \"mantis\",\n",
        "    \"cicada\",\n",
        "    \"leafhopper\",\n",
        "    \"lacewing\",\n",
        "    \"dragonfly\",\n",
        "    \"damselfly\",\n",
        "    \"admiral\",\n",
        "    \"ringlet\",\n",
        "    \"monarch\",\n",
        "    \"cabbage butterfly\",\n",
        "    \"sulphur butterfly\",\n",
        "    \"lycaenid\",\n",
        "    \"starfish\",\n",
        "    \"sea urchin\",\n",
        "    \"sea cucumber\",\n",
        "    \"wood rabbit\",\n",
        "    \"hare\",\n",
        "    \"Angora\",\n",
        "    \"hamster\",\n",
        "    \"porcupine\",\n",
        "    \"fox squirrel\",\n",
        "    \"marmot\",\n",
        "    \"beaver\",\n",
        "    \"guinea pig\",\n",
        "    \"sorrel\",\n",
        "    \"zebra\",\n",
        "    \"hog\",\n",
        "    \"wild boar\",\n",
        "    \"warthog\",\n",
        "    \"hippopotamus\",\n",
        "    \"ox\",\n",
        "    \"water buffalo\",\n",
        "    \"bison\",\n",
        "    \"ram\",\n",
        "    \"bighorn\",\n",
        "    \"ibex\",\n",
        "    \"hartebeest\",\n",
        "    \"impala\",\n",
        "    \"gazelle\",\n",
        "    \"Arabian camel\",\n",
        "    \"llama\",\n",
        "    \"weasel\",\n",
        "    \"mink\",\n",
        "    \"polecat\",\n",
        "    \"black-footed ferret\",\n",
        "    \"otter\",\n",
        "    \"skunk\",\n",
        "    \"badger\",\n",
        "    \"armadillo\",\n",
        "    \"three-toed sloth\",\n",
        "    \"orangutan\",\n",
        "    \"gorilla\",\n",
        "    \"chimpanzee\",\n",
        "    \"gibbon\",\n",
        "    \"siamang\",\n",
        "    \"guenon\",\n",
        "    \"patas\",\n",
        "    \"baboon\",\n",
        "    \"macaque\",\n",
        "    \"langur\",\n",
        "    \"colobus\",\n",
        "    \"proboscis monkey\",\n",
        "    \"marmoset\",\n",
        "    \"capuchin\",\n",
        "    \"howler monkey\",\n",
        "    \"titi\",\n",
        "    \"spider monkey\",\n",
        "    \"squirrel monkey\",\n",
        "    \"Madagascar cat\",\n",
        "    \"indri\",\n",
        "    \"Indian elephant\",\n",
        "    \"African elephant\",\n",
        "    \"lesser panda\",\n",
        "    \"giant panda\",\n",
        "    \"barracouta\",\n",
        "    \"eel\",\n",
        "    \"coho\",\n",
        "    \"rock beauty\",\n",
        "    \"anemone fish\",\n",
        "    \"sturgeon\",\n",
        "    \"gar\",\n",
        "    \"lionfish\",\n",
        "    \"puffer\",\n",
        "    \"abacus\",\n",
        "    \"abaya\",\n",
        "    \"academic gown\",\n",
        "    \"accordion\",\n",
        "    \"acoustic guitar\",\n",
        "    \"aircraft carrier\",\n",
        "    \"airliner\",\n",
        "    \"airship\",\n",
        "    \"altar\",\n",
        "    \"ambulance\",\n",
        "    \"amphibian\",\n",
        "    \"analog clock\",\n",
        "    \"apiary\",\n",
        "    \"apron\",\n",
        "    \"ashcan\",\n",
        "    \"assault rifle\",\n",
        "    \"backpack\",\n",
        "    \"bakery\",\n",
        "    \"balance beam\",\n",
        "    \"balloon\",\n",
        "    \"ballpoint\",\n",
        "    \"Band Aid\",\n",
        "    \"banjo\",\n",
        "    \"bannister\",\n",
        "    \"barbell\",\n",
        "    \"barber chair\",\n",
        "    \"barbershop\",\n",
        "    \"barn\",\n",
        "    \"barometer\",\n",
        "    \"barrel\",\n",
        "    \"barrow\",\n",
        "    \"baseball\",\n",
        "    \"basketball\",\n",
        "    \"bassinet\",\n",
        "    \"bassoon\",\n",
        "    \"bathing cap\",\n",
        "    \"bath towel\",\n",
        "    \"bathtub\",\n",
        "    \"beach wagon\",\n",
        "    \"beacon\",\n",
        "    \"beaker\",\n",
        "    \"bearskin\",\n",
        "    \"beer bottle\",\n",
        "    \"beer glass\",\n",
        "    \"bell cote\",\n",
        "    \"bib\",\n",
        "    \"bicycle-built-for-two\",\n",
        "    \"bikini\",\n",
        "    \"binder\",\n",
        "    \"binoculars\",\n",
        "    \"birdhouse\",\n",
        "    \"boathouse\",\n",
        "    \"bobsled\",\n",
        "    \"bolo tie\",\n",
        "    \"bonnet\",\n",
        "    \"bookcase\",\n",
        "    \"bookshop\",\n",
        "    \"bottlecap\",\n",
        "    \"bow\",\n",
        "    \"bow tie\",\n",
        "    \"brass\",\n",
        "    \"brassiere\",\n",
        "    \"breakwater\",\n",
        "    \"breastplate\",\n",
        "    \"broom\",\n",
        "    \"bucket\",\n",
        "    \"buckle\",\n",
        "    \"bulletproof vest\",\n",
        "    \"bullet train\",\n",
        "    \"butcher shop\",\n",
        "    \"cab\",\n",
        "    \"caldron\",\n",
        "    \"candle\",\n",
        "    \"cannon\",\n",
        "    \"canoe\",\n",
        "    \"can opener\",\n",
        "    \"cardigan\",\n",
        "    \"car mirror\",\n",
        "    \"carousel\",\n",
        "    \"carpenter's kit\",\n",
        "    \"carton\",\n",
        "    \"car wheel\",\n",
        "    \"cash machine\",\n",
        "    \"cassette\",\n",
        "    \"cassette player\",\n",
        "    \"castle\",\n",
        "    \"catamaran\",\n",
        "    \"CD player\",\n",
        "    \"cello\",\n",
        "    \"cellular telephone\",\n",
        "    \"chain\",\n",
        "    \"chainlink fence\",\n",
        "    \"chain mail\",\n",
        "    \"chain saw\",\n",
        "    \"chest\",\n",
        "    \"chiffonier\",\n",
        "    \"chime\",\n",
        "    \"china cabinet\",\n",
        "    \"Christmas stocking\",\n",
        "    \"church\",\n",
        "    \"cinema\",\n",
        "    \"cleaver\",\n",
        "    \"cliff dwelling\",\n",
        "    \"cloak\",\n",
        "    \"clog\",\n",
        "    \"cocktail shaker\",\n",
        "    \"coffee mug\",\n",
        "    \"coffeepot\",\n",
        "    \"coil\",\n",
        "    \"combination lock\",\n",
        "    \"computer keyboard\",\n",
        "    \"confectionery\",\n",
        "    \"container ship\",\n",
        "    \"convertible\",\n",
        "    \"corkscrew\",\n",
        "    \"cornet\",\n",
        "    \"cowboy boot\",\n",
        "    \"cowboy hat\",\n",
        "    \"cradle\",\n",
        "    \"crane\",\n",
        "    \"crash helmet\",\n",
        "    \"crate\",\n",
        "    \"crib\",\n",
        "    \"Crock Pot\",\n",
        "    \"croquet ball\",\n",
        "    \"crutch\",\n",
        "    \"cuirass\",\n",
        "    \"dam\",\n",
        "    \"desk\",\n",
        "    \"desktop computer\",\n",
        "    \"dial telephone\",\n",
        "    \"diaper\",\n",
        "    \"digital clock\",\n",
        "    \"digital watch\",\n",
        "    \"dining table\",\n",
        "    \"dishrag\",\n",
        "    \"dishwasher\",\n",
        "    \"disk brake\",\n",
        "    \"dock\",\n",
        "    \"dogsled\",\n",
        "    \"dome\",\n",
        "    \"doormat\",\n",
        "    \"drilling platform\",\n",
        "    \"drum\",\n",
        "    \"drumstick\",\n",
        "    \"dumbbell\",\n",
        "    \"Dutch oven\",\n",
        "    \"electric fan\",\n",
        "    \"electric guitar\",\n",
        "    \"electric locomotive\",\n",
        "    \"entertainment center\",\n",
        "    \"envelope\",\n",
        "    \"espresso maker\",\n",
        "    \"face powder\",\n",
        "    \"feather boa\",\n",
        "    \"file\",\n",
        "    \"fireboat\",\n",
        "    \"fire engine\",\n",
        "    \"fire screen\",\n",
        "    \"flagpole\",\n",
        "    \"flute\",\n",
        "    \"folding chair\",\n",
        "    \"football helmet\",\n",
        "    \"forklift\",\n",
        "    \"fountain\",\n",
        "    \"fountain pen\",\n",
        "    \"four-poster\",\n",
        "    \"freight car\",\n",
        "    \"French horn\",\n",
        "    \"frying pan\",\n",
        "    \"fur coat\",\n",
        "    \"garbage truck\",\n",
        "    \"gasmask\",\n",
        "    \"gas pump\",\n",
        "    \"goblet\",\n",
        "    \"go-kart\",\n",
        "    \"golf ball\",\n",
        "    \"golfcart\",\n",
        "    \"gondola\",\n",
        "    \"gong\",\n",
        "    \"gown\",\n",
        "    \"grand piano\",\n",
        "    \"greenhouse\",\n",
        "    \"grille\",\n",
        "    \"grocery store\",\n",
        "    \"guillotine\",\n",
        "    \"hair slide\",\n",
        "    \"hair spray\",\n",
        "    \"half track\",\n",
        "    \"hammer\",\n",
        "    \"hamper\",\n",
        "    \"hand blower\",\n",
        "    \"hand-held computer\",\n",
        "    \"handkerchief\",\n",
        "    \"hard disc\",\n",
        "    \"harmonica\",\n",
        "    \"harp\",\n",
        "    \"harvester\",\n",
        "    \"hatchet\",\n",
        "    \"holster\",\n",
        "    \"home theater\",\n",
        "    \"honeycomb\",\n",
        "    \"hook\",\n",
        "    \"hoopskirt\",\n",
        "    \"horizontal bar\",\n",
        "    \"horse cart\",\n",
        "    \"hourglass\",\n",
        "    \"iPod\",\n",
        "    \"iron\",\n",
        "    \"jack-o'-lantern\",\n",
        "    \"jean\",\n",
        "    \"jeep\",\n",
        "    \"jersey\",\n",
        "    \"jigsaw puzzle\",\n",
        "    \"jinrikisha\",\n",
        "    \"joystick\",\n",
        "    \"kimono\",\n",
        "    \"knee pad\",\n",
        "    \"knot\",\n",
        "    \"lab coat\",\n",
        "    \"ladle\",\n",
        "    \"lampshade\",\n",
        "    \"laptop\",\n",
        "    \"lawn mower\",\n",
        "    \"lens cap\",\n",
        "    \"letter opener\",\n",
        "    \"library\",\n",
        "    \"lifeboat\",\n",
        "    \"lighter\",\n",
        "    \"limousine\",\n",
        "    \"liner\",\n",
        "    \"lipstick\",\n",
        "    \"Loafer\",\n",
        "    \"lotion\",\n",
        "    \"loudspeaker\",\n",
        "    \"loupe\",\n",
        "    \"lumbermill\",\n",
        "    \"magnetic compass\",\n",
        "    \"mailbag\",\n",
        "    \"mailbox\",\n",
        "    \"maillot\",\n",
        "    \"maillot tank suit\",\n",
        "    \"manhole cover\",\n",
        "    \"maraca\",\n",
        "    \"marimba\",\n",
        "    \"mask\",\n",
        "    \"matchstick\",\n",
        "    \"maypole\",\n",
        "    \"maze\",\n",
        "    \"measuring cup\",\n",
        "    \"medicine chest\",\n",
        "    \"megalith\",\n",
        "    \"microphone\",\n",
        "    \"microwave\",\n",
        "    \"military uniform\",\n",
        "    \"milk can\",\n",
        "    \"minibus\",\n",
        "    \"miniskirt\",\n",
        "    \"minivan\",\n",
        "    \"missile\",\n",
        "    \"mitten\",\n",
        "    \"mixing bowl\",\n",
        "    \"mobile home\",\n",
        "    \"Model T\",\n",
        "    \"modem\",\n",
        "    \"monastery\",\n",
        "    \"monitor\",\n",
        "    \"moped\",\n",
        "    \"mortar\",\n",
        "    \"mortarboard\",\n",
        "    \"mosque\",\n",
        "    \"mosquito net\",\n",
        "    \"motor scooter\",\n",
        "    \"mountain bike\",\n",
        "    \"mountain tent\",\n",
        "    \"mouse\",\n",
        "    \"mousetrap\",\n",
        "    \"moving van\",\n",
        "    \"muzzle\",\n",
        "    \"nail\",\n",
        "    \"neck brace\",\n",
        "    \"necklace\",\n",
        "    \"nipple\",\n",
        "    \"notebook\",\n",
        "    \"obelisk\",\n",
        "    \"oboe\",\n",
        "    \"ocarina\",\n",
        "    \"odometer\",\n",
        "    \"oil filter\",\n",
        "    \"organ\",\n",
        "    \"oscilloscope\",\n",
        "    \"overskirt\",\n",
        "    \"oxcart\",\n",
        "    \"oxygen mask\",\n",
        "    \"packet\",\n",
        "    \"paddle\",\n",
        "    \"paddlewheel\",\n",
        "    \"padlock\",\n",
        "    \"paintbrush\",\n",
        "    \"pajama\",\n",
        "    \"palace\",\n",
        "    \"panpipe\",\n",
        "    \"paper towel\",\n",
        "    \"parachute\",\n",
        "    \"parallel bars\",\n",
        "    \"park bench\",\n",
        "    \"parking meter\",\n",
        "    \"passenger car\",\n",
        "    \"patio\",\n",
        "    \"pay-phone\",\n",
        "    \"pedestal\",\n",
        "    \"pencil box\",\n",
        "    \"pencil sharpener\",\n",
        "    \"perfume\",\n",
        "    \"Petri dish\",\n",
        "    \"photocopier\",\n",
        "    \"pick\",\n",
        "    \"pickelhaube\",\n",
        "    \"picket fence\",\n",
        "    \"pickup\",\n",
        "    \"pier\",\n",
        "    \"piggy bank\",\n",
        "    \"pill bottle\",\n",
        "    \"pillow\",\n",
        "    \"ping-pong ball\",\n",
        "    \"pinwheel\",\n",
        "    \"pirate\",\n",
        "    \"pitcher\",\n",
        "    \"plane\",\n",
        "    \"planetarium\",\n",
        "    \"plastic bag\",\n",
        "    \"plate rack\",\n",
        "    \"plow\",\n",
        "    \"plunger\",\n",
        "    \"Polaroid camera\",\n",
        "    \"pole\",\n",
        "    \"police van\",\n",
        "    \"poncho\",\n",
        "    \"pool table\",\n",
        "    \"pop bottle\",\n",
        "    \"pot\",\n",
        "    \"potter's wheel\",\n",
        "    \"power drill\",\n",
        "    \"prayer rug\",\n",
        "    \"printer\",\n",
        "    \"prison\",\n",
        "    \"projectile\",\n",
        "    \"projector\",\n",
        "    \"puck\",\n",
        "    \"punching bag\",\n",
        "    \"purse\",\n",
        "    \"quill\",\n",
        "    \"quilt\",\n",
        "    \"racer\",\n",
        "    \"racket\",\n",
        "    \"radiator\",\n",
        "    \"radio\",\n",
        "    \"radio telescope\",\n",
        "    \"rain barrel\",\n",
        "    \"recreational vehicle\",\n",
        "    \"reel\",\n",
        "    \"reflex camera\",\n",
        "    \"refrigerator\",\n",
        "    \"remote control\",\n",
        "    \"restaurant\",\n",
        "    \"revolver\",\n",
        "    \"rifle\",\n",
        "    \"rocking chair\",\n",
        "    \"rotisserie\",\n",
        "    \"rubber eraser\",\n",
        "    \"rugby ball\",\n",
        "    \"rule\",\n",
        "    \"running shoe\",\n",
        "    \"safe\",\n",
        "    \"safety pin\",\n",
        "    \"saltshaker\",\n",
        "    \"sandal\",\n",
        "    \"sarong\",\n",
        "    \"sax\",\n",
        "    \"scabbard\",\n",
        "    \"scale\",\n",
        "    \"school bus\",\n",
        "    \"schooner\",\n",
        "    \"scoreboard\",\n",
        "    \"screen\",\n",
        "    \"screw\",\n",
        "    \"screwdriver\",\n",
        "    \"seat belt\",\n",
        "    \"sewing machine\",\n",
        "    \"shield\",\n",
        "    \"shoe shop\",\n",
        "    \"shoji\",\n",
        "    \"shopping basket\",\n",
        "    \"shopping cart\",\n",
        "    \"shovel\",\n",
        "    \"shower cap\",\n",
        "    \"shower curtain\",\n",
        "    \"ski\",\n",
        "    \"ski mask\",\n",
        "    \"sleeping bag\",\n",
        "    \"slide rule\",\n",
        "    \"sliding door\",\n",
        "    \"slot\",\n",
        "    \"snorkel\",\n",
        "    \"snowmobile\",\n",
        "    \"snowplow\",\n",
        "    \"soap dispenser\",\n",
        "    \"soccer ball\",\n",
        "    \"sock\",\n",
        "    \"solar dish\",\n",
        "    \"sombrero\",\n",
        "    \"soup bowl\",\n",
        "    \"space bar\",\n",
        "    \"space heater\",\n",
        "    \"space shuttle\",\n",
        "    \"spatula\",\n",
        "    \"speedboat\",\n",
        "    \"spider web\",\n",
        "    \"spindle\",\n",
        "    \"sports car\",\n",
        "    \"spotlight\",\n",
        "    \"stage\",\n",
        "    \"steam locomotive\",\n",
        "    \"steel arch bridge\",\n",
        "    \"steel drum\",\n",
        "    \"stethoscope\",\n",
        "    \"stole\",\n",
        "    \"stone wall\",\n",
        "    \"stopwatch\",\n",
        "    \"stove\",\n",
        "    \"strainer\",\n",
        "    \"streetcar\",\n",
        "    \"stretcher\",\n",
        "    \"studio couch\",\n",
        "    \"stupa\",\n",
        "    \"submarine\",\n",
        "    \"suit\",\n",
        "    \"sundial\",\n",
        "    \"sunglass\",\n",
        "    \"sunglasses\",\n",
        "    \"sunscreen\",\n",
        "    \"suspension bridge\",\n",
        "    \"swab\",\n",
        "    \"sweatshirt\",\n",
        "    \"swimming trunks\",\n",
        "    \"swing\",\n",
        "    \"switch\",\n",
        "    \"syringe\",\n",
        "    \"table lamp\",\n",
        "    \"tank\",\n",
        "    \"tape player\",\n",
        "    \"teapot\",\n",
        "    \"teddy\",\n",
        "    \"television\",\n",
        "    \"tennis ball\",\n",
        "    \"thatch\",\n",
        "    \"theater curtain\",\n",
        "    \"thimble\",\n",
        "    \"thresher\",\n",
        "    \"throne\",\n",
        "    \"tile roof\",\n",
        "    \"toaster\",\n",
        "    \"tobacco shop\",\n",
        "    \"toilet seat\",\n",
        "    \"torch\",\n",
        "    \"totem pole\",\n",
        "    \"tow truck\",\n",
        "    \"toyshop\",\n",
        "    \"tractor\",\n",
        "    \"trailer truck\",\n",
        "    \"tray\",\n",
        "    \"trench coat\",\n",
        "    \"tricycle\",\n",
        "    \"trimaran\",\n",
        "    \"tripod\",\n",
        "    \"triumphal arch\",\n",
        "    \"trolleybus\",\n",
        "    \"trombone\",\n",
        "    \"tub\",\n",
        "    \"turnstile\",\n",
        "    \"typewriter keyboard\",\n",
        "    \"umbrella\",\n",
        "    \"unicycle\",\n",
        "    \"upright\",\n",
        "    \"vacuum\",\n",
        "    \"vase\",\n",
        "    \"vault\",\n",
        "    \"velvet\",\n",
        "    \"vending machine\",\n",
        "    \"vestment\",\n",
        "    \"viaduct\",\n",
        "    \"violin\",\n",
        "    \"volleyball\",\n",
        "    \"waffle iron\",\n",
        "    \"wall clock\",\n",
        "    \"wallet\",\n",
        "    \"wardrobe\",\n",
        "    \"warplane\",\n",
        "    \"washbasin\",\n",
        "    \"washer\",\n",
        "    \"water bottle\",\n",
        "    \"water jug\",\n",
        "    \"water tower\",\n",
        "    \"whiskey jug\",\n",
        "    \"whistle\",\n",
        "    \"wig\",\n",
        "    \"window screen\",\n",
        "    \"window shade\",\n",
        "    \"Windsor tie\",\n",
        "    \"wine bottle\",\n",
        "    \"wing\",\n",
        "    \"wok\",\n",
        "    \"wooden spoon\",\n",
        "    \"wool\",\n",
        "    \"worm fence\",\n",
        "    \"wreck\",\n",
        "    \"yawl\",\n",
        "    \"yurt\",\n",
        "    \"web site\",\n",
        "    \"comic book\",\n",
        "    \"crossword puzzle\",\n",
        "    \"street sign\",\n",
        "    \"traffic light\",\n",
        "    \"book jacket\",\n",
        "    \"menu\",\n",
        "    \"plate\",\n",
        "    \"guacamole\",\n",
        "    \"consomme\",\n",
        "    \"hot pot\",\n",
        "    \"trifle\",\n",
        "    \"ice cream\",\n",
        "    \"ice lolly\",\n",
        "    \"French loaf\",\n",
        "    \"bagel\",\n",
        "    \"pretzel\",\n",
        "    \"cheeseburger\",\n",
        "    \"hotdog\",\n",
        "    \"mashed potato\",\n",
        "    \"head cabbage\",\n",
        "    \"broccoli\",\n",
        "    \"cauliflower\",\n",
        "    \"zucchini\",\n",
        "    \"spaghetti squash\",\n",
        "    \"acorn squash\",\n",
        "    \"butternut squash\",\n",
        "    \"cucumber\",\n",
        "    \"artichoke\",\n",
        "    \"bell pepper\",\n",
        "    \"cardoon\",\n",
        "    \"mushroom\",\n",
        "    \"Granny Smith\",\n",
        "    \"strawberry\",\n",
        "    \"orange\",\n",
        "    \"lemon\",\n",
        "    \"fig\",\n",
        "    \"pineapple\",\n",
        "    \"banana\",\n",
        "    \"jackfruit\",\n",
        "    \"custard apple\",\n",
        "    \"pomegranate\",\n",
        "    \"hay\",\n",
        "    \"carbonara\",\n",
        "    \"chocolate sauce\",\n",
        "    \"dough\",\n",
        "    \"meat loaf\",\n",
        "    \"pizza\",\n",
        "    \"potpie\",\n",
        "    \"burrito\",\n",
        "    \"red wine\",\n",
        "    \"espresso\",\n",
        "    \"cup\",\n",
        "    \"eggnog\",\n",
        "    \"alp\",\n",
        "    \"bubble\",\n",
        "    \"cliff\",\n",
        "    \"coral reef\",\n",
        "    \"geyser\",\n",
        "    \"lakeside\",\n",
        "    \"promontory\",\n",
        "    \"sandbar\",\n",
        "    \"seashore\",\n",
        "    \"valley\",\n",
        "    \"volcano\",\n",
        "    \"ballplayer\",\n",
        "    \"groom\",\n",
        "    \"scuba diver\",\n",
        "    \"rapeseed\",\n",
        "    \"daisy\",\n",
        "    \"yellow lady's slipper\",\n",
        "    \"corn\",\n",
        "    \"acorn\",\n",
        "    \"hip\",\n",
        "    \"buckeye\",\n",
        "    \"coral fungus\",\n",
        "    \"agaric\",\n",
        "    \"gyromitra\",\n",
        "    \"stinkhorn\",\n",
        "    \"earthstar\",\n",
        "    \"hen-of-the-woods\",\n",
        "    \"bolete\",\n",
        "    \"ear\",\n",
        "    \"toilet tissue\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lEyGH44JkswX"
      },
      "outputs": [],
      "source": [
        "# @title Utils\n",
        "def _log_api_usage_once(obj: Any) -> None:\n",
        "\n",
        "    \"\"\"\n",
        "    Logs API usage(module and name) within an organization.\n",
        "    In a large ecosystem, it's often useful to track the PyTorch and\n",
        "    TorchVision APIs usage. This API provides the similar functionality to the\n",
        "    logging module in the Python stdlib. It can be used for debugging purpose\n",
        "    to log which methods are used and by default it is inactive, unless the user\n",
        "    manually subscribes a logger via the `SetAPIUsageLogger method\n",
        "    <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n",
        "    Please note it is triggered only once for the same API call within a process.\n",
        "    It does not collect any data from open-source users since it is no-op by default.\n",
        "    For more information, please refer to\n",
        "    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n",
        "    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n",
        "\n",
        "    Args:\n",
        "        obj (class instance or method): an object to extract info from.\n",
        "    \"\"\"\n",
        "    module = obj.__module__\n",
        "    if not module.startswith(\"torchvision\"):\n",
        "        module = f\"torchvision.internal.{module}\"\n",
        "    name = obj.__class__.__name__\n",
        "    if isinstance(obj, FunctionType):\n",
        "        name = obj.__name__\n",
        "    torch._C._log_api_usage_once(f\"{module}.{name}\")\n",
        "\n",
        "@dataclass\n",
        "class Weights:\n",
        "    \"\"\"\n",
        "    This class is used to group important attributes associated with the pre-trained weights.\n",
        "\n",
        "    Args:\n",
        "        url (str): The location where we find the weights.\n",
        "        transforms (Callable): A callable that constructs the preprocessing method (or validation preset transforms)\n",
        "            needed to use the model. The reason we attach a constructor method rather than an already constructed\n",
        "            object is because the specific object might have memory and thus we want to delay initialization until\n",
        "            needed.\n",
        "        meta (Dict[str, Any]): Stores meta-data related to the weights of the model and its configuration. These can be\n",
        "            informative attributes (for example the number of parameters/flops, recipe link/methods used in training\n",
        "            etc), configuration parameters (for example the `num_classes`) needed to construct the model or important\n",
        "            meta-data (for example the `classes` of a classification model) needed to use the model.\n",
        "    \"\"\"\n",
        "\n",
        "    url: str\n",
        "    transforms: Callable\n",
        "    meta: Dict[str, Any]\n",
        "\n",
        "    def __eq__(self, other: Any) -> bool:\n",
        "        # We need this custom implementation for correct deep-copy and deserialization behavior.\n",
        "        # TL;DR: After the definition of an enum, creating a new instance, i.e. by deep-copying or deserializing it,\n",
        "        # involves an equality check against the defined members. Unfortunately, the `transforms` attribute is often\n",
        "        # defined with `functools.partial` and `fn = partial(...); assert deepcopy(fn) != fn`. Without custom handling\n",
        "        # for it, the check against the defined members would fail and effectively prevent the weights from being\n",
        "        # deep-copied or deserialized.\n",
        "        # See https://github.com/pytorch/vision/pull/7107 for details.\n",
        "        if not isinstance(other, Weights):\n",
        "            return NotImplemented\n",
        "\n",
        "        if self.url != other.url:\n",
        "            return False\n",
        "\n",
        "        if self.meta != other.meta:\n",
        "            return False\n",
        "\n",
        "        if isinstance(self.transforms, partial) and isinstance(other.transforms, partial):\n",
        "            return (\n",
        "                self.transforms.func == other.transforms.func\n",
        "                and self.transforms.args == other.transforms.args\n",
        "                and self.transforms.keywords == other.transforms.keywords\n",
        "            )\n",
        "        else:\n",
        "            return self.transforms == other.transforms\n",
        "\n",
        "class WeightsEnum(Enum):\n",
        "    \"\"\"\n",
        "    This class is the parent class of all model weights. Each model building method receives an optional `weights`\n",
        "    parameter with its associated pre-trained weights. It inherits from `Enum` and its values should be of type\n",
        "    `Weights`.\n",
        "\n",
        "    Args:\n",
        "        value (Weights): The data class entry with the weight information.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def verify(cls, obj: Any) -> Any:\n",
        "        if obj is not None:\n",
        "            if type(obj) is str:\n",
        "                obj = cls[obj.replace(cls.__name__ + \".\", \"\")]\n",
        "            elif not isinstance(obj, cls):\n",
        "                raise TypeError(\n",
        "                    f\"Invalid Weight class provided; expected {cls.__name__} but received {obj.__class__.__name__}.\"\n",
        "                )\n",
        "        return obj\n",
        "\n",
        "    def get_state_dict(self, *args: Any, **kwargs: Any) -> Mapping[str, Any]:\n",
        "        return load_state_dict_from_url(self.url, *args, **kwargs)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.__class__.__name__}.{self._name_}\"\n",
        "\n",
        "    @property\n",
        "    def url(self):\n",
        "        return self.value.url\n",
        "\n",
        "    @property\n",
        "    def transforms(self):\n",
        "        return self.value.transforms\n",
        "\n",
        "    @property\n",
        "    def meta(self):\n",
        "        return self.value.meta\n",
        "\n",
        "V = TypeVar(\"V\")\n",
        "def _ovewrite_named_param(kwargs: Dict[str, Any], param: str, new_value: V) -> None:\n",
        "    if param in kwargs:\n",
        "        if kwargs[param] != new_value:\n",
        "            raise ValueError(f\"The parameter '{param}' expected value {new_value} but got {kwargs[param]} instead.\")\n",
        "    else:\n",
        "        kwargs[param] = new_value\n",
        "\n",
        "def _load_state_dict(model: nn.Module, weights: WeightsEnum, progress: bool) -> None:\n",
        "    \"\"\"\n",
        "    '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    They are also in the checkpoints in model_urls. This pattern is used\n",
        "    to find such keys.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(\n",
        "        r\"^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\"\n",
        "    )\n",
        "\n",
        "    state_dict = weights.get_state_dict(progress=progress, check_hash=True)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "class InterpolationMode(Enum):\n",
        "    \"\"\"Interpolation modes\n",
        "    Available interpolation methods are ``nearest``, ``nearest-exact``, ``bilinear``, ``bicubic``, ``box``, ``hamming``,\n",
        "    and ``lanczos``.\n",
        "    \"\"\"\n",
        "\n",
        "    NEAREST = \"nearest\"\n",
        "    NEAREST_EXACT = \"nearest-exact\"\n",
        "    BILINEAR = \"bilinear\"\n",
        "    BICUBIC = \"bicubic\"\n",
        "    # For PIL compatibility\n",
        "    BOX = \"box\"\n",
        "    HAMMING = \"hamming\"\n",
        "    LANCZOS = \"lanczos\"\n",
        "\n",
        "class ImageClassification(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        crop_size: int,\n",
        "        resize_size: int = 256,\n",
        "        mean: Tuple[float, ...] = (0.485, 0.456, 0.406),\n",
        "        std: Tuple[float, ...] = (0.229, 0.224, 0.225),\n",
        "        interpolation: InterpolationMode = InterpolationMode.BILINEAR,\n",
        "        antialias: Optional[bool] = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.crop_size = [crop_size]\n",
        "        self.resize_size = [resize_size]\n",
        "        self.mean = list(mean)\n",
        "        self.std = list(std)\n",
        "        self.interpolation = interpolation\n",
        "        self.antialias = antialias\n",
        "\n",
        "    def forward(self, img: Tensor) -> Tensor:\n",
        "        img = F.resize(img, self.resize_size, interpolation=self.interpolation, antialias=self.antialias)\n",
        "        img = F.center_crop(img, self.crop_size)\n",
        "        if not isinstance(img, Tensor):\n",
        "            img = F.pil_to_tensor(img)\n",
        "        img = F.convert_image_dtype(img, torch.float)\n",
        "        img = F.normalize(img, mean=self.mean, std=self.std)\n",
        "        return img\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        format_string = self.__class__.__name__ + \"(\"\n",
        "        format_string += f\"\\n    crop_size={self.crop_size}\"\n",
        "        format_string += f\"\\n    resize_size={self.resize_size}\"\n",
        "        format_string += f\"\\n    mean={self.mean}\"\n",
        "        format_string += f\"\\n    std={self.std}\"\n",
        "        format_string += f\"\\n    interpolation={self.interpolation}\"\n",
        "        format_string += \"\\n)\"\n",
        "        return format_string\n",
        "\n",
        "    def describe(self) -> str:\n",
        "        return (\n",
        "            \"Accepts ``PIL.Image``, batched ``(B, C, H, W)`` and single ``(C, H, W)`` image ``torch.Tensor`` objects. \"\n",
        "            f\"The images are resized to ``resize_size={self.resize_size}`` using ``interpolation={self.interpolation}``, \"\n",
        "            f\"followed by a central crop of ``crop_size={self.crop_size}``. Finally the values are first rescaled to \"\n",
        "            f\"``[0.0, 1.0]`` and then normalized using ``mean={self.mean}`` and ``std={self.std}``.\"\n",
        "        )\n",
        "\n",
        "_COMMON_META = {\n",
        "    \"min_size\": (29, 29),\n",
        "    \"categories\": _IMAGENET_CATEGORIES,\n",
        "    \"recipe\": \"https://github.com/pytorch/vision/pull/116\",\n",
        "    \"_docs\": \"\"\"These weights are ported from LuaTorch.\"\"\",\n",
        "}\n",
        "\n",
        "class DenseNet121_Weights(WeightsEnum):\n",
        "    IMAGENET1K_V1 = Weights(\n",
        "        url=\"https://download.pytorch.org/models/densenet121-a639ec97.pth\",\n",
        "        transforms=partial(ImageClassification, crop_size=224),\n",
        "        meta={\n",
        "            **_COMMON_META,\n",
        "            \"num_params\": 7978856,\n",
        "            \"_metrics\": {\n",
        "                \"ImageNet-1K\": {\n",
        "                    \"acc@1\": 74.434,\n",
        "                    \"acc@5\": 91.972,\n",
        "                }\n",
        "            },\n",
        "            \"_ops\": 2.834,\n",
        "            \"_file_size\": 30.845,\n",
        "        },\n",
        "    )\n",
        "    DEFAULT = IMAGENET1K_V1\n",
        "\n",
        "\n",
        "def _densenet(\n",
        "    growth_rate: int,\n",
        "    block_config: Tuple[int, int, int, int],\n",
        "    num_init_features: int,\n",
        "    weights: Optional[WeightsEnum],\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        ") -> DenseNet:\n",
        "\n",
        "    if weights is not None:\n",
        "      _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    if weights is not None:\n",
        "      _load_state_dict(model=model, weights=weights, progress=progress)\n",
        "    return model\n",
        "\n",
        "def densenet121(*, weights: Optional[DenseNet121_Weights] = None, progress: bool = True, **kwargs: Any) -> DenseNet:\n",
        "    \"\"\"\n",
        "    Densenet-121 model\n",
        "    Args:\n",
        "        weights (:class:`~torchvision.models.DenseNet121_Weights`, optional): The\n",
        "            pretrained weights to use. See\n",
        "            :class:`~torchvision.models.DenseNet121_Weights` below for\n",
        "            more details, and possible values. By default, no pre-trained\n",
        "            weights are used.\n",
        "        progress (bool, optional): If True, displays a progress bar of the download to stderr. Default is True.\n",
        "        **kwargs: parameters passed to the ``torchvision.models.densenet.DenseNet`` base class.\n",
        "    \"\"\"\n",
        "    weights = DenseNet121_Weights.verify(weights)\n",
        "    return _densenet(32, (6, 12, 24, 16), 64, weights, progress, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79gwLN1MUguo"
      },
      "source": [
        "## GoogleNet\n",
        "\n",
        "The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection. [Paper](https://arxiv.org/pdf/1409.4842)\n",
        "\n",
        "#### Inception Module\n",
        "<img src=\"https://i.ibb.co/b6TTddX/image.png\" alt=\"image\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvBatchRelu(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                          stride=stride, padding=padding, bias=False)\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.relu(self.bn(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lGwwI-rKj4lk"
      },
      "outputs": [],
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, out1x1, in3x3, out3x3, in5x5, out5x5, pool_proj, num_classes=None, aux=False):\n",
        "        super().__init__()\n",
        "        self.branch1 = ConvBatchRelu(in_channels, out1x1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            ConvBatchRelu(in_channels, in3x3, kernel_size=1, stride=1, padding=0),\n",
        "            ConvBatchRelu(in3x3, out3x3, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            ConvBatchRelu(in_channels, in5x5, kernel_size=1, stride=1, padding=0),\n",
        "            ConvBatchRelu(in5x5, out5x5, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n",
        "            ConvBatchRelu(in_channels, pool_proj, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "        self.aux = aux\n",
        "        self.num_classes = num_classes\n",
        "        if aux == True:\n",
        "          self.aux_branch = nn.Sequential(\n",
        "              nn.AvgPool2d(kernel_size=5, stride=3),\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1 = self.branch1(x)\n",
        "        branch2 = self.branch2(x)\n",
        "        branch3 = self.branch3(x)\n",
        "        branch4 = self.branch4(x)\n",
        "        out = [branch1, branch2, branch3, branch4]\n",
        "        out = torch.cat(out, 1)\n",
        "\n",
        "        if self.aux == True:\n",
        "          aux_out = self.aux_branch(x)\n",
        "          aux_out = torch.flatten(aux_out, 1)\n",
        "          out2 = nn.Linear(aux_out.size(1), self.num_classes)\n",
        "          aux_out = out2(aux_out)\n",
        "          return out, aux_out\n",
        "\n",
        "        else: \n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwzJ1_PRkGdI"
      },
      "source": [
        "#### Architecture\n",
        "\n",
        "<img src=\"https://i.ibb.co/vLb6ys5/image-2024-06-13-100528355.png\" alt=\"image-2024-06-13-100528355\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HeBrb62iUkO6"
      },
      "outputs": [],
      "source": [
        "class Stem(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = ConvBatchRelu(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "    self.mp1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.norm = ConvBatchRelu(64, 64, kernel_size=1, stride=1)\n",
        "    self.conv2 = ConvBatchRelu(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "    self.mp2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.mp1(self.conv1(x))\n",
        "    x = self.norm(x)\n",
        "    return self.mp2(self.conv2(x))\n",
        "\n",
        "class GGNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super().__init__()\n",
        "    self.stem = Stem()\n",
        "\n",
        "    # Inception Module 3\n",
        "    self.inception3a = Inception(in_channels=192, out1x1=64,\n",
        "                                 in3x3=96, out3x3=128,\n",
        "                                 in5x5=16, out5x5=32,\n",
        "                                 pool_proj=32)\n",
        "\n",
        "    self.inception3b = Inception(in_channels=256, out1x1=128,\n",
        "                                 in3x3=128, out3x3=192,\n",
        "                                 in5x5=32, out5x5=96,\n",
        "                                 pool_proj=64)\n",
        "\n",
        "    self.mp1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # Inception Module 4\n",
        "    self.inception4a = Inception(in_channels=480, out1x1=192,\n",
        "                                 in3x3=96, out3x3=208,\n",
        "                                 in5x5=16, out5x5=48,\n",
        "                                 pool_proj=64)\n",
        "\n",
        "    self.inception4b = Inception(in_channels=512, out1x1=160,\n",
        "                                 in3x3=112, out3x3=224,\n",
        "                                 in5x5=24, out5x5=64,\n",
        "                                 pool_proj=64, num_classes=num_classes, aux=True)\n",
        "\n",
        "    self.inception4c = Inception(in_channels=512, out1x1=128,\n",
        "                                 in3x3=128, out3x3=256,\n",
        "                                 in5x5=24, out5x5=64,\n",
        "                                 pool_proj=64)\n",
        "\n",
        "    self.inception4d = Inception(in_channels=512, out1x1=112,\n",
        "                                 in3x3=144, out3x3=288,\n",
        "                                 in5x5=32, out5x5=64,\n",
        "                                 pool_proj=64)\n",
        "\n",
        "    self.inception4e = Inception(in_channels=528, out1x1=256,\n",
        "                                 in3x3=160, out3x3=320,\n",
        "                                 in5x5=32, out5x5=128,\n",
        "                                 pool_proj=128, num_classes=num_classes, aux=True)\n",
        "\n",
        "    self.mp2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    # Inception Module 5\n",
        "    self.inception5a = Inception(in_channels=832, out1x1=256,\n",
        "                                 in3x3=160, out3x3=320,\n",
        "                                 in5x5=32, out5x5=128,\n",
        "                                 pool_proj=128)\n",
        "\n",
        "    self.inception5b = Inception(in_channels=832, out1x1=384,\n",
        "                                 in3x3=192, out3x3=384,\n",
        "                                 in5x5=48, out5x5=128,\n",
        "                                 pool_proj=128)\n",
        "\n",
        "    # Output\n",
        "    self.avg_pool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
        "    self.dropout = nn.Dropout(0.1, True)\n",
        "    self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.stem(x)\n",
        "    x = self.inception3a(x)\n",
        "    x = self.inception3b(x)\n",
        "    x = self.mp1(x)\n",
        "\n",
        "    x = self.inception4a(x)\n",
        "    x, aux_4b = self.inception4b(x)\n",
        "    x = self.inception4c(x)\n",
        "    x = self.inception4d(x)\n",
        "    x, aux_4e = self.inception4e(x)\n",
        "    x = self.mp2(x)\n",
        "\n",
        "    x = self.inception5a(x)\n",
        "    x = self.inception5b(x)\n",
        "\n",
        "    x = self.avg_pool(x)\n",
        "    x = self.dropout(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc(x)\n",
        "    return x, aux_4b, aux_4e\n",
        "\n",
        "x = torch.randn(12, 3, 224, 224)\n",
        "model = GGNet(100)\n",
        "y, aux1, aux2 = model(x)\n",
        "print(y.shape, aux1.shape, aux2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyHfYtT3UkWQ"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "A residual learning framework to ease the training\n",
        "of networks that are substantially deeper than those used\n",
        "previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Comprehensive empirical evidence shows that these residual\n",
        "networks are easier to optimize, and can gain accuracy from\n",
        "considerably increased depth. On the ImageNet dataset we\n",
        "evaluate residual nets with a depth of up to 152 layers8x\n",
        "deeper than VGG nets but still having lower complexity.An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis\n",
        "on CIFAR-10 with 100 and 1000 layers. [Paper](https://arxiv.org/pdf/1512.03385)\n",
        "\n",
        "<img width=\"1200\" src=\"https://i.ibb.co/W5rf2cF/image.png\" alt=\"image\" border=\"0\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simplified version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "snS5EKvPubOL"
      },
      "outputs": [],
      "source": [
        "class Bridge(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, stride, downsample=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv2 = conv3x3(out_channels, out_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    x = self.bn2(self.conv2(self.bn1(self.conv1(x))))\n",
        "    if self.downsample != None:\n",
        "      residual = self.downsample(residual)\n",
        "    return self.relu(residual + x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, num_blocks, stride):\n",
        "    super().__init__()\n",
        "\n",
        "    # Check if not 1st Block\n",
        "    downsample=None\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "        downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2, bias=False) # conv1x1\n",
        "\n",
        "    # First Bridge\n",
        "    layers = []\n",
        "    layers.append(Bridge(in_channels, out_channels, stride, downsample))\n",
        "    in_channels = out_channels\n",
        "\n",
        "    # Other Bridges\n",
        "    for _ in range(1, num_blocks):\n",
        "      layers.append(Bridge(in_channels, out_channels, stride=1))\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class _ResNet(nn.Module):\n",
        "  def __init__(self, num_classes=100):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feature Extraction\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    # Blocks\n",
        "    self.block1 = Block(in_channels=64, out_channels=64, num_blocks=3, stride=1)\n",
        "    self.block2 = Block(in_channels=64, out_channels=128, num_blocks=4, stride=2)\n",
        "    self.block3 = Block(in_channels=128, out_channels=256, num_blocks=6, stride=2)\n",
        "    self.block4 = Block(in_channels=256, out_channels=512, num_blocks=3, stride=2)\n",
        "\n",
        "    # Output\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Feature Extraction\n",
        "    x = self.maxpool(self.relu(self.bn1(self.conv1(x))))\n",
        "\n",
        "    # Blocks\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "\n",
        "    # Output\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torchvision version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2e7s14AU2m5X"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
        "    This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "    \"\"\"\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        _log_api_usage_once(self)\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            \"\"\" each element in the tuple indicates if we should replace\n",
        "            the 2x2 stride with a dilated convolution instead \"\"\"\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "        dilate: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(\n",
        "    block: Type[Union[BasicBlock, Bottleneck]],\n",
        "    layers: List[int],\n",
        "    weights: Optional[WeightsEnum],\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        ") -> ResNet:\n",
        "\n",
        "    if weights is not None:\n",
        "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if weights is not None:\n",
        "        model.load_state_dict(weights.get_state_dict(progress=progress, check_hash=True))\n",
        "    return model\n",
        "\n",
        "\n",
        "_COMMON_META = {\n",
        "    \"min_size\": (1, 1),\n",
        "    \"categories\": _IMAGENET_CATEGORIES,\n",
        "}\n",
        "\n",
        "\n",
        "class ResNet50_Weights(WeightsEnum):\n",
        "    IMAGENET1K_V1 = Weights(\n",
        "        url=\"https://download.pytorch.org/models/resnet50-0676ba61.pth\",\n",
        "        transforms=partial(ImageClassification, crop_size=224),\n",
        "        meta={\n",
        "            **_COMMON_META,\n",
        "            \"num_params\": 25557032,\n",
        "            \"recipe\": \"https://github.com/pytorch/vision/tree/main/references/classification#resnet\",\n",
        "            \"_metrics\": {\n",
        "                \"ImageNet-1K\": {\n",
        "                    \"acc@1\": 76.130,\n",
        "                    \"acc@5\": 92.862,\n",
        "                }\n",
        "            },\n",
        "            \"_ops\": 4.089,\n",
        "            \"_file_size\": 97.781,\n",
        "            \"_docs\": \"\"\"These weights reproduce closely the results of the paper using a simple training recipe.\"\"\",\n",
        "        },\n",
        "    )\n",
        "    IMAGENET1K_V2 = Weights(\n",
        "        url=\"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\",\n",
        "        transforms=partial(ImageClassification, crop_size=224, resize_size=232),\n",
        "        meta={\n",
        "            **_COMMON_META,\n",
        "            \"num_params\": 25557032,\n",
        "            \"recipe\": \"https://github.com/pytorch/vision/issues/3995#issuecomment-1013906621\",\n",
        "            \"_metrics\": {\n",
        "                \"ImageNet-1K\": {\n",
        "                    \"acc@1\": 80.858,\n",
        "                    \"acc@5\": 95.434,\n",
        "                }\n",
        "            },\n",
        "            \"_ops\": 4.089,\n",
        "            \"_file_size\": 97.79,\n",
        "            \"_docs\": \"\"\"\n",
        "                These weights improve upon the results of the original paper by using TorchVision's `new training recipe\n",
        "                <https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/>`_.\n",
        "            \"\"\",\n",
        "        },\n",
        "    )\n",
        "    DEFAULT = IMAGENET1K_V2\n",
        "\n",
        "def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n",
        "    weights = ResNet50_Weights.verify(weights)\n",
        "    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoARvzxSUoUQ"
      },
      "source": [
        "## DenseNet\n",
        "\n",
        "Dense Convolutional Network (DenseNet) connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with $L$ layers have $L$ connectionsone between each layer and its subsequent layer, our network has $\\frac{L(L+1)}{2}$ direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. [Paper](https://arxiv.org/pdf/1608.06993)\n",
        "\n",
        "<center>\n",
        "<img width=\"700\" src=\"https://i.ibb.co/7GsdcsZ/image-2024-06-13-141410503.png\" alt=\"image-2024-06-13-141410503\" border=\"0\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simplied version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "06v7k2EHCnan"
      },
      "outputs": [],
      "source": [
        "class FeatureExtraction(nn.Module):\n",
        "  def __init__(self, num_init_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(3, num_init_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.norm = nn.BatchNorm2d(num_init_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.pool(self.relu(self.norm(self.conv(x))))\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "  def __init__(self, input_channels, growth_rate, bn_size, drop_rate):\n",
        "    super().__init__()\n",
        "    fix_channel = bn_size * growth_rate\n",
        "    self.norm1 = nn.BatchNorm2d(input_channels)\n",
        "    self.relu1 = nn.ReLU(inplace=True)\n",
        "    self.conv1 = nn.Conv2d(input_channels, fix_channel, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "    self.norm2 = nn.BatchNorm2d(fix_channel)\n",
        "    self.relu2 = nn.ReLU(inplace=True)\n",
        "    self.conv2 = nn.Conv2d(fix_channel, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    self.drop_rate = float(drop_rate)\n",
        "\n",
        "  def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
        "        return bottleneck_output\n",
        "\n",
        "  def forward(self, input):\n",
        "      if isinstance(input, Tensor): prev_features = [input]\n",
        "      else: prev_features = input\n",
        "      bottleneck_output = self.bn_function(prev_features)\n",
        "      new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "      return F.dropout(new_features, p=self.drop_rate)\n",
        "\n",
        "class DenseBlock(nn.ModuleDict):\n",
        "  def __init__(self, num_layers, input_channels, bn_size, growth_rate, drop_rate=0.1):\n",
        "    super().__init__()\n",
        "    for i in range(num_layers):\n",
        "      layer = DenseLayer(input_channels + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "      self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
        "\n",
        "  def forward(self, init_features):\n",
        "    features = [init_features]\n",
        "    for name, layer in self.items():\n",
        "        new_features = layer(features)\n",
        "        features.append(new_features)\n",
        "    return torch.cat(features, 1)\n",
        "\n",
        "class Transition(nn.Sequential):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super().__init__()\n",
        "    self.norm = nn.BatchNorm2d(input_features)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.conv = nn.Conv2d(input_features, output_features, kernel_size=1, stride=1, bias=False)\n",
        "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.pool(self.conv(self.relu(self.norm(x))))\n",
        "\n",
        "class SimplifiedDenseNet(nn.Module):\n",
        "  def __init__(self, num_classes, growth_rate=32, num_init_channels=64,\n",
        "               layer_each_block=(6, 12, 24, 16), bn_size=4, drop_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feature extraction\n",
        "    self.feature_extraction = FeatureExtraction(num_init_channels)\n",
        "\n",
        "    # DenseBlocks\n",
        "    num_channels = num_init_channels\n",
        "    self.denseblock1 = DenseBlock(layer_each_block[0], num_channels, bn_size, growth_rate)\n",
        "    concated_channels = num_channels + layer_each_block[0] * growth_rate\n",
        "    self.transition1 = Transition(concated_channels, concated_channels//2)\n",
        "\n",
        "    num_channels *= 2\n",
        "    self.denseblock2 = DenseBlock(layer_each_block[1], num_channels, bn_size, growth_rate)\n",
        "    concated_channels = num_channels + layer_each_block[1] * growth_rate\n",
        "    self.transition2 = Transition(concated_channels, concated_channels//2)\n",
        "\n",
        "    num_channels *= 2\n",
        "    self.denseblock3 = DenseBlock(layer_each_block[2], num_channels, bn_size, growth_rate)\n",
        "    concated_channels = num_channels + layer_each_block[2] * growth_rate\n",
        "    self.transition3 = Transition(concated_channels, concated_channels//2)\n",
        "\n",
        "    num_channels *= 2\n",
        "    self.denseblock4 = DenseBlock(layer_each_block[3], num_channels, bn_size, growth_rate)\n",
        "\n",
        "    # Output\n",
        "    self.final_norm = nn.BatchNorm2d(num_channels*2)\n",
        "    self.final_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.classifier = nn.Linear(num_channels*2, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_extraction(x)\n",
        "\n",
        "    x = self.denseblock1(x)\n",
        "    x = self.transition1(x)\n",
        "\n",
        "    x = self.denseblock2(x)\n",
        "    x = self.transition2(x)\n",
        "\n",
        "    x = self.denseblock3(x)\n",
        "    x = self.transition3(x)\n",
        "\n",
        "    x = self.denseblock4(x)\n",
        "    x = self.final_pool(self.final_norm(x))\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.classifier(x)\n",
        "\n",
        "x = torch.randn(12, 3, 224, 224)\n",
        "model = SimplifiedDenseNet(num_classes=1000)\n",
        "y = model(x)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LUA1kUsdsXA"
      },
      "source": [
        "The DenseNet architecture is highly computationally efficient as a result of\n",
        "feature reuse. However, a nave DenseNet implementation can require a significant amount of GPU memory: If not properly managed, pre-activation batch normalization and contiguous convolution operations can produce feature maps that grow quadratically with network depth. This implementation follows the strategy of shared memory allocations to reduce the memory cost for storing feature maps from quadratic to linear.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/PjxygZD/image.png\" alt=\"image\" border=\"0\">\n",
        "<img src=\"https://i.ibb.co/G207C92/image.png\" alt=\"image\" border=\"0\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torchvision version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uExl1mfwiGO3"
      },
      "outputs": [],
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features: int, growth_rate: int, bn_size: int,\n",
        "                 drop_rate: float, memory_efficient: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs: List[Tensor]) -> Tensor:\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        return bottleneck_output\n",
        "\n",
        "    # todo: rewrite when torchscript supports any\n",
        "    def any_requires_grad(self, input: List[Tensor]) -> bool:\n",
        "        for tensor in input:\n",
        "            if tensor.requires_grad:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    @torch.jit.unused  # noqa: T484\n",
        "    def call_checkpoint_bottleneck(self, input: List[Tensor]) -> Tensor:\n",
        "        def closure(*inputs):\n",
        "            return self.bn_function(inputs)\n",
        "\n",
        "        return cp.checkpoint(closure, *input, use_reentrant=False)\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input: List[Tensor]) -> Tensor:  # noqa: F811\n",
        "        pass\n",
        "\n",
        "    @torch.jit._overload_method  # noqa: F811\n",
        "    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\n",
        "        pass\n",
        "\n",
        "    # torchscript does not yet support *args, so we overload method\n",
        "    # allowing it to take either a List[Tensor] or single Tensor\n",
        "    def forward(self, input: Tensor) -> Tensor:  # noqa: F811\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "        if self.memory_efficient and self.any_requires_grad(prev_features):\n",
        "            if torch.jit.is_scripting():\n",
        "                raise Exception(\"Memory Efficient not supported in JIT\")\n",
        "\n",
        "            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n",
        "        else:\n",
        "            bottleneck_output = self.bn_function(prev_features)\n",
        "\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        num_input_features: int,\n",
        "        bn_size: int,\n",
        "        growth_rate: int,\n",
        "        drop_rate: float,\n",
        "        memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module(\"denselayer%d\" % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features: Tensor) -> Tensor:\n",
        "        print(\"New Block ===\")\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            print()\n",
        "            print(f\"Feature: {new_features.shape}\")\n",
        "            print()\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm2d(num_input_features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Densely Connected Convolutional Networks\n",
        "\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "            (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n",
        "        but slower. Default: *False*. See `\"paper\" <https://arxiv.org/pdf/1707.06990.pdf>`_.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 24, 16),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0,\n",
        "        num_classes: int = 1000,\n",
        "        memory_efficient: bool = False,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        _log_api_usage_once(self)\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
        "                    (\"relu0\", nn.ReLU(inplace=True)),\n",
        "                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module(\"transition%d\" % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        features = self.features(x)\n",
        "        print(features.shape)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torchvision User Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWuVyKbWwHC_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained DenseNet121 model\n",
        "model = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\", progress=True)\n",
        "model.eval()\n",
        "\n",
        "# Define the image preprocessing steps\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load an image\n",
        "img_path = 'dog.png'  # Replace with the path to your image\n",
        "img = Image.open(img_path)\n",
        "img_t = preprocess(img)\n",
        "input_tensor = img_t.unsqueeze(0)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "# Print the top 5 most probable classes\n",
        "_, indices = torch.topk(probabilities, 5)\n",
        "print(f'Top 5 classes: {_IMAGENET_CATEGORIES[indices]}')\n",
        "print(f'Probabilities: {probabilities[indices]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwOfqf3TWoXB"
      },
      "source": [
        "# MobileNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4ztUYykzyx"
      },
      "source": [
        "## Seperable Convolution\n",
        "\n",
        "<img src=\"https://i.ibb.co/4scJtB9/image.png\" alt=\"image\" border=\"0\">\n",
        "\n",
        "Input feature maps $X$: $D_{in} \\times D_{in} \\times C_{in}$, 1 convolution kernel: $D_k \\times D_k \\times C_{in} \\rightarrow$ output feature maps $G$: $D_{out} \\times D_{out} \\times N$. So the cost:\n",
        "- Mults once: $D_K^2 \\times C_{in}$\n",
        "- Mults per conv kernel: $D_K^2 \\times C_{in} \\times D_{out}^2$\n",
        "- Mults $C_{out}$ kernels: $D_K^2 \\times C_{in} \\times D_{out}^2 \\times C_{out} \\space (2)$\n",
        "\n",
        "Depthwise Convolution: Filtering stage + Pointwise ConvolutionL Combining stage.\n",
        "- Instead of using $D_{in} \\times D_{in} \\times C_{in}$, we use $M$ kernels $D_{in} \\times D_{in} \\times 1$. With each this kernel, we end up having $D_{out} \\times D_{out} \\times 1$, so stacking $M$ this output, we get $D_{out} \\times D_{out} \\times C_{in}$. So we have the cost: $D_K^2 \\times D_{out}^2 \\times C_{in}$\n",
        "- We use $C_{out}$ convs $1 \\times 1 \\times C_{in}$, so we end up changing the channel size from $C_{in}$ to $C_{out}$. So we have the cost: $C_{in} \\times D_{out}^2 \\times C_{out}$\n",
        "- Total cost is: $D_K^2 \\times D_{out}^2 \\times M + M \\times D_{out}^2 \\times N = M \\times D_{out}^2 \\times (D_K^2 + C_{out}) \\space (1)$\n",
        "\n",
        "So finally, we take:\n",
        "$$(1) / (2) = \\frac{D_K^2 + C_{out}}{D_K^2 \\times C_{out}} = \\frac{1}{C_{out}} + \\frac{1}{D_K^2}$$\n",
        "\n",
        "If $N=1024$ and $D_K=3$ (common), we get: $$\\frac{1}{1024} + \\frac{1}{9} = 0.112$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C5RD1dEayoi",
        "outputId": "61f7bd33-5f45-48d5-9717-c230a82557f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1024, 64, 64]) torch.Size([1, 1024, 64, 64])\n",
            "19331547136 4719616\n",
            "2172649472 530432\n",
            "Params: 0.11238880451290953\n",
            "FLOP: 0.11238880451290953\n"
          ]
        }
      ],
      "source": [
        "class StandardConv(nn.Module):\n",
        "  def __init__(self, C_in, C_out, K):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=K, stride=2, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class SeperableConv(nn.Module):\n",
        "  def __init__(self, C_in, C_out, K):\n",
        "    super().__init__()\n",
        "    self.depthwise = nn.Conv2d(in_channels=C_in, out_channels=C_in, kernel_size=K, groups=C_in, stride=2, padding=1)\n",
        "    self.pointwise = nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.depthwise(x)\n",
        "    x = self.pointwise(x)\n",
        "    return x\n",
        "\n",
        "x = torch.randn(1, 512, 128, 128)\n",
        "\n",
        "standard_conv = StandardConv(C_in=512, C_out=1024, K=3)\n",
        "standard_x = standard_conv(x)\n",
        "seperable_conv = SeperableConv(C_in=512, C_out=1024, K=3)\n",
        "sep_x = seperable_conv(x)\n",
        "\n",
        "print(standard_x.shape, sep_x.shape)\n",
        "\n",
        "output = torchinfo.summary(standard_conv, x.shape)\n",
        "print(output.total_mult_adds, output.total_params)\n",
        "\n",
        "output_sep = torchinfo.summary(seperable_conv, x.shape)\n",
        "print(output_sep.total_mult_adds, output_sep.total_params)\n",
        "\n",
        "print(f\"Params: {output_sep.total_params / output.total_params}\")\n",
        "print(f\"FLOP: {output_sep.total_mult_adds / output.total_mult_adds}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STflIwmc13PZ"
      },
      "source": [
        "# ShuffleNet\n",
        "The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy.\n",
        "\n",
        "<img src=\"https://i.ibb.co/3sCBhBP/image.png\" alt=\"image\" border=\"0\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4mnkmMI9Xdi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "from torch.nn import init\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1,\n",
        "            padding=1, bias=True, groups=1):\n",
        "    \"\"\"3x3 convolution with padding \"\"\"\n",
        "    return nn.Conv2d(in_channels,\n",
        "                     out_channels,\n",
        "                     kernel_size=3,\n",
        "                     stride=stride,\n",
        "                     padding=padding,\n",
        "                     bias=bias,\n",
        "                     groups=groups)\n",
        "\n",
        "def conv1x1(in_channels, out_channels, groups=1):\n",
        "    \"\"\"1x1 convolution with padding\n",
        "    - Normal pointwise convolution When groups == 1\n",
        "    - Grouped pointwise convolution when groups > 1\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(in_channels,\n",
        "                     out_channels,\n",
        "                     kernel_size=1,\n",
        "                     groups=groups,\n",
        "                     stride=1)\n",
        "\n",
        "\n",
        "def channel_shuffle(x, groups):\n",
        "    batchsize, num_channels, height, width = x.data.size()\n",
        "    channels_per_group = num_channels // groups\n",
        "\n",
        "    # reshape\n",
        "    x = x.view(batchsize, groups, channels_per_group, height, width)\n",
        "\n",
        "    # transpose\n",
        "    # - contiguous() required if transpose() is used before view().\n",
        "    #   See https://github.com/pytorch/pytorch/issues/764\n",
        "    x = torch.transpose(x, 1, 2).contiguous()\n",
        "\n",
        "    # flatten\n",
        "    x = x.view(batchsize, -1, height, width)\n",
        "    return x\n",
        "\n",
        "\n",
        "class ShuffleUnit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, groups=3,\n",
        "                 grouped_conv=True, combine='add'):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.grouped_conv = grouped_conv\n",
        "        self.combine = combine\n",
        "        self.groups = groups\n",
        "        self.bottleneck_channels = self.out_channels // 4\n",
        "\n",
        "        # Define the type of ShuffleUnit\n",
        "        if self.combine == 'add':\n",
        "            # ShuffleUnit Figure 2b\n",
        "            self.depthwise_stride = 1\n",
        "            self._combine_func = self._add\n",
        "\n",
        "        elif self.combine == 'concat':\n",
        "            # ShuffleUnit Figure 2c\n",
        "            self.depthwise_stride = 2\n",
        "            self._combine_func = self._concat\n",
        "            self.out_channels -= self.in_channels\n",
        "        else:\n",
        "            raise ValueError(\"Cannot combine tensors with \\\"{}\\\"\" \\\n",
        "                             \"Only \\\"add\\\" and \\\"concat\\\" are\" \\\n",
        "                             \"supported\".format(self.combine))\n",
        "\n",
        "        # Use a 1x1 grouped or non-grouped convolution to reduce input channels\n",
        "        # to bottleneck channels, as in a ResNet bottleneck module.\n",
        "        # NOTE: Do not use group convolution for the first conv1x1 in Stage 2.\n",
        "        self.first_1x1_groups = self.groups if grouped_conv else 1\n",
        "\n",
        "        self.g_conv_1x1_compress = self._make_grouped_conv1x1(\n",
        "            self.in_channels,\n",
        "            self.bottleneck_channels,\n",
        "            self.first_1x1_groups,\n",
        "            batch_norm=True,\n",
        "            relu=True\n",
        "        )\n",
        "\n",
        "        # 3x3 depthwise convolution followed by batch normalization\n",
        "        self.depthwise_conv3x3 = conv3x3(\n",
        "            self.bottleneck_channels, self.bottleneck_channels,\n",
        "            stride=self.depthwise_stride, groups=self.bottleneck_channels\n",
        "        )\n",
        "        self.bn_after_depthwise = nn.BatchNorm2d(self.bottleneck_channels)\n",
        "\n",
        "        # Use 1x1 grouped convolution to expand from bottleneck_channels to out_channels\n",
        "        self.g_conv_1x1_expand = self._make_grouped_conv1x1(\n",
        "            self.bottleneck_channels,\n",
        "            self.out_channels,\n",
        "            self.groups,\n",
        "            batch_norm=True,\n",
        "            relu=False\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _add(x, out):\n",
        "        # residual connection\n",
        "        return x + out\n",
        "\n",
        "    @staticmethod\n",
        "    def _concat(x, out):\n",
        "        # concatenate along channel axis\n",
        "        return torch.cat((x, out), 1)\n",
        "\n",
        "    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n",
        "                              batch_norm=True, relu=False):\n",
        "        modules = OrderedDict()\n",
        "        conv = conv1x1(in_channels, out_channels, groups=groups)\n",
        "        modules['conv1x1'] = conv\n",
        "\n",
        "        if batch_norm:\n",
        "            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n",
        "        if relu:\n",
        "            modules['relu'] = nn.ReLU()\n",
        "        if len(modules) > 1:\n",
        "            return nn.Sequential(modules)\n",
        "        else:\n",
        "            return conv\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # save for combining later with output\n",
        "        print()\n",
        "        print(\"Shuffle Unit\")\n",
        "        print(f\"Input: {x.shape}\")\n",
        "        residual = x\n",
        "\n",
        "        if self.combine == 'concat':\n",
        "            residual = F.avg_pool2d(residual, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        out = self.g_conv_1x1_compress(x)\n",
        "        print(f\"out after g_conv_1x1_compress: {out.shape} \")\n",
        "        out = channel_shuffle(out, self.groups)\n",
        "        print(f\"out after channel_shuffle: {out.shape} \")\n",
        "        out = self.depthwise_conv3x3(out)\n",
        "        print(f\"out after depthwise_conv3x3: {out.shape} \")\n",
        "        out = self.bn_after_depthwise(out)\n",
        "        print(f\"out after bn_after_depthwise: {out.shape} \")\n",
        "        out = self.g_conv_1x1_expand(out)\n",
        "        print(f\"out after g_conv_1x1_expand: {out.shape} \")\n",
        "\n",
        "        out = self._combine_func(residual, out)\n",
        "        return F.relu(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpOY_rtgol1"
      },
      "source": [
        "<img src=\"https://i.ibb.co/QCfc6kr/image.png\" alt=\"image\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qplcn9T9grSz"
      },
      "outputs": [],
      "source": [
        "class ShuffleNet(nn.Module):\n",
        "    def __init__(self, groups=3, in_channels=3, num_classes=1000):\n",
        "        \"\"\"\n",
        "        ShuffleNet constructor.\n",
        "        Arguments:\n",
        "            groups (int, optional): number of groups to be used in grouped\n",
        "                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n",
        "                performance according to original paper.\n",
        "            in_channels (int, optional): number of channels in the input tensor.\n",
        "                Default is 3 for RGB image inputs.\n",
        "            num_classes (int, optional): number of classes to predict. Default\n",
        "                is 1000 for ImageNet.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.groups = groups\n",
        "        self.stage_repeats = [3, 7, 3]\n",
        "        self.in_channels =  in_channels\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if groups == 1:\n",
        "            self.stage_out_channels = [-1, 24, 144, 288, 567]\n",
        "        elif groups == 2:\n",
        "            self.stage_out_channels = [-1, 24, 200, 400, 800]\n",
        "        elif groups == 3:\n",
        "            self.stage_out_channels = [-1, 24, 240, 480, 960]\n",
        "        elif groups == 4:\n",
        "            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n",
        "        elif groups == 8:\n",
        "            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"\"\"{} groups is not supported for\n",
        "                   1x1 Grouped Convolutions\"\"\".format(groups))\n",
        "\n",
        "        # Stage 1 always has 24 output channels\n",
        "        self.conv1 = conv3x3(self.in_channels, self.stage_out_channels[1], stride=2)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Stage 2-3-4\n",
        "        self.stage2 = self._make_stage(2)\n",
        "        self.stage3 = self._make_stage(3)\n",
        "        self.stage4 = self._make_stage(4)\n",
        "\n",
        "        # Fully-connected classification layer\n",
        "        num_inputs = self.stage_out_channels[-1]\n",
        "        self.fc = nn.Linear(num_inputs, self.num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_stage(self, stage):\n",
        "        modules = OrderedDict()\n",
        "        stage_name = \"ShuffleUnit_Stage{}\".format(stage)\n",
        "\n",
        "        \"\"\"\n",
        "        First ShuffleUnit in the stage\n",
        "        1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n",
        "           is used in Stage 2. Group convolutions used everywhere else. \"\"\"\n",
        "\n",
        "        grouped_conv = stage > 2\n",
        "\n",
        "        # 2. concatenation unit is always used.\n",
        "        first_module = ShuffleUnit(\n",
        "            self.stage_out_channels[stage-1],\n",
        "            self.stage_out_channels[stage],\n",
        "            groups=self.groups,\n",
        "            grouped_conv=grouped_conv,\n",
        "            combine='concat'\n",
        "        )\n",
        "        modules[stage_name+\"_0\"] = first_module\n",
        "\n",
        "        # add more ShuffleUnits depending on pre-defined number of repeats\n",
        "        for i in range(self.stage_repeats[stage-2]):\n",
        "            name = stage_name + \"_{}\".format(i+1)\n",
        "            module = ShuffleUnit(\n",
        "                self.stage_out_channels[stage],\n",
        "                self.stage_out_channels[stage],\n",
        "                groups=self.groups,\n",
        "                grouped_conv=True,\n",
        "                combine='add'\n",
        "            )\n",
        "            modules[name] = module\n",
        "        return nn.Sequential(modules)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stage 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Other stage\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "\n",
        "        # Output\n",
        "        x = F.avg_pool2d(x, x.data.size()[-2:])\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "input = torch.rand(3, 3, 224, 224)\n",
        "model = ShuffleNet()\n",
        "output = model(input)\n",
        "output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uApXYB2txVsM"
      },
      "source": [
        "# RepVGG\n",
        "\n",
        "<img src=\"https://i.ibb.co/wrxq7Kv/image.png\" alt=\"image\" border=\"0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reparameterization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOmI77Q3-vhU",
        "outputId": "f0f46ea4-d34f-4de8-92df-1b388fe2cfea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 5.4000,  9.3000,  9.9000, 11.2000,  6.7000],\n",
            "         [13.6000, 13.9000, 12.8000, 11.2000,  6.8000],\n",
            "         [17.6000, 14.6000, 12.6000, 13.6000,  6.1000],\n",
            "         [ 9.0000, 15.4000, 19.1000, 15.5000,  8.1000],\n",
            "         [10.6000, 13.8000, 16.9000, 18.9000, 12.6000]]])\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [2, 3, 5, 1, 2],\n",
        "    [9, 8, 1, 5, 4],\n",
        "    [0, 1, 2, 4, 0],\n",
        "    [5, 6, 7, 8, 9],\n",
        "]).unsqueeze(0).to(torch.float32)\n",
        "\n",
        "weight_1 = torch.tensor([\n",
        "    [0.5, 0.1, 0.2],\n",
        "    [0.2, 0.3, 0.9],\n",
        "    [0.1, 0.4, 0.6],\n",
        "]).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "weight_2 = torch.tensor([\n",
        "    [0.7]\n",
        "]).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "out1 = F.conv2d(x, weight_1, stride=1, padding=1)\n",
        "out2 = F.conv2d(x, weight_2, stride=1, padding=0)\n",
        "temp = out1 + out2\n",
        "print(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf-aM-z8ILMm",
        "outputId": "16086dbe-02bb-400d-93ec-4db0e149bec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(13.9000)\n"
          ]
        }
      ],
      "source": [
        "x_flat = torch.tensor([1, 2, 3, 2, 3, 5, 9, 8, 1]).to(torch.float32)\n",
        "weight_1 = weight_1.view(-1)\n",
        "s = 0.7 * 3\n",
        "\n",
        "print(x_flat @ weight_1 + s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y70UzYK-CU2U",
        "outputId": "077397df-4be7-49bc-de05-73860bee4b6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 5.4000,  9.3000,  9.9000, 11.2000,  6.7000],\n",
              "         [13.6000, 13.9000, 12.8000, 11.2000,  6.8000],\n",
              "         [17.6000, 14.6000, 12.6000, 13.6000,  6.1000],\n",
              "         [ 9.0000, 15.4000, 19.1000, 15.5000,  8.1000],\n",
              "         [10.6000, 13.8000, 16.9000, 18.9000, 12.6000]]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weight_1 = torch.tensor([\n",
        "    [0.5, 0.1, 0.2],\n",
        "    [0.2, 0.3, 0.9],\n",
        "    [0.1, 0.4, 0.6],\n",
        "]).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "weight_2 = torch.tensor([\n",
        "    [0., 0., 0.],\n",
        "    [0., 0.7, 0.],\n",
        "    [0., 0., 0.],\n",
        "]).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "weight = weight_1 + weight_2\n",
        "\n",
        "out = F.conv2d(x, weight, stride=1, padding=1)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Torchvision version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQxlnAduxbnr"
      },
      "outputs": [],
      "source": [
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1):\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding, groups=groups, bias=False))\n",
        "    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))\n",
        "    return result\n",
        "\n",
        "class RepVGGBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):\n",
        "        super(RepVGGBlock, self).__init__()\n",
        "        self.deploy = deploy\n",
        "        self.groups = groups\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        assert kernel_size == 3\n",
        "        assert padding == 1\n",
        "\n",
        "        padding_11 = padding - kernel_size // 2\n",
        "\n",
        "        self.nonlinearity = nn.ReLU()\n",
        "\n",
        "        if use_se:\n",
        "            #   Note that RepVGG-D2se uses SE before nonlinearity. But RepVGGplus models uses SE after nonlinearity.\n",
        "            self.se = SEBlock(out_channels, internal_neurons=out_channels // 16)\n",
        "        else:\n",
        "            self.se = nn.Identity()\n",
        "\n",
        "        if deploy:\n",
        "            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                                      padding=padding, dilation=dilation, groups=groups, bias=True, padding_mode=padding_mode)\n",
        "\n",
        "        else:\n",
        "            self.rbr_identity = nn.BatchNorm2d(num_features=in_channels) if out_channels == in_channels and stride == 1 else None\n",
        "            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, groups=groups)\n",
        "            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, padding=padding_11, groups=groups)\n",
        "            print('RepVGG Block, identity = ', self.rbr_identity)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))\n",
        "\n",
        "        if self.rbr_identity is None:\n",
        "            id_out = 0\n",
        "        else:\n",
        "            id_out = self.rbr_identity(inputs)\n",
        "\n",
        "        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))\n",
        "\n",
        "\n",
        "    #   Optional. This may improve the accuracy and facilitates quantization in some cases.\n",
        "    #   1.  Cancel the original weight decay on rbr_dense.conv.weight and rbr_1x1.conv.weight.\n",
        "    #   2.  Use like this.\n",
        "    #       loss = criterion(....)\n",
        "    #       for every RepVGGBlock blk:\n",
        "    #           loss += weight_decay_coefficient * 0.5 * blk.get_cust_L2()\n",
        "    #       optimizer.zero_grad()\n",
        "    #       loss.backward()\n",
        "    def get_custom_L2(self):\n",
        "        K3 = self.rbr_dense.conv.weight\n",
        "        K1 = self.rbr_1x1.conv.weight\n",
        "        t3 = (self.rbr_dense.bn.weight / ((self.rbr_dense.bn.running_var + self.rbr_dense.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
        "        t1 = (self.rbr_1x1.bn.weight / ((self.rbr_1x1.bn.running_var + self.rbr_1x1.bn.eps).sqrt())).reshape(-1, 1, 1, 1).detach()\n",
        "\n",
        "        l2_loss_circle = (K3 ** 2).sum() - (K3[:, :, 1:2, 1:2] ** 2).sum()      # The L2 loss of the \"circle\" of weights in 3x3 kernel. Use regular L2 on them.\n",
        "        eq_kernel = K3[:, :, 1:2, 1:2] * t3 + K1 * t1                           # The equivalent resultant central point of 3x3 kernel.\n",
        "        l2_loss_eq_kernel = (eq_kernel ** 2 / (t3 ** 2 + t1 ** 2)).sum()        # Normalize for an L2 coefficient comparable to regular L2.\n",
        "        return l2_loss_eq_kernel + l2_loss_circle\n",
        "\n",
        "\n",
        "\n",
        "#   This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
        "#   You can get the equivalent kernel and bias at any time and do whatever you want,\n",
        "    #   for example, apply some penalties or constraints during training, just like you do to the other models.\n",
        "#   May be useful for quantization or pruning.\n",
        "    def get_equivalent_kernel_bias(self):\n",
        "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
        "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
        "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
        "        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid\n",
        "\n",
        "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
        "        if kernel1x1 is None:\n",
        "            return 0\n",
        "        else:\n",
        "            return torch.nn.functional.pad(kernel1x1, [1,1,1,1])\n",
        "\n",
        "    def _fuse_bn_tensor(self, branch):\n",
        "        if branch is None:\n",
        "            return 0, 0\n",
        "        if isinstance(branch, nn.Sequential):\n",
        "            kernel = branch.conv.weight\n",
        "            running_mean = branch.bn.running_mean\n",
        "            running_var = branch.bn.running_var\n",
        "            gamma = branch.bn.weight\n",
        "            beta = branch.bn.bias\n",
        "            eps = branch.bn.eps\n",
        "        else:\n",
        "            assert isinstance(branch, nn.BatchNorm2d)\n",
        "            if not hasattr(self, 'id_tensor'):\n",
        "                input_dim = self.in_channels // self.groups\n",
        "                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)\n",
        "                for i in range(self.in_channels):\n",
        "                    kernel_value[i, i % input_dim, 1, 1] = 1\n",
        "                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)\n",
        "            kernel = self.id_tensor\n",
        "            running_mean = branch.running_mean\n",
        "            running_var = branch.running_var\n",
        "            gamma = branch.weight\n",
        "            beta = branch.bias\n",
        "            eps = branch.eps\n",
        "        std = (running_var + eps).sqrt()\n",
        "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
        "        return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "    def switch_to_deploy(self):\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            return\n",
        "        kernel, bias = self.get_equivalent_kernel_bias()\n",
        "        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels, out_channels=self.rbr_dense.conv.out_channels,\n",
        "                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,\n",
        "                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation, groups=self.rbr_dense.conv.groups, bias=True)\n",
        "        self.rbr_reparam.weight.data = kernel\n",
        "        self.rbr_reparam.bias.data = bias\n",
        "        self.__delattr__('rbr_dense')\n",
        "        self.__delattr__('rbr_1x1')\n",
        "        if hasattr(self, 'rbr_identity'):\n",
        "            self.__delattr__('rbr_identity')\n",
        "        if hasattr(self, 'id_tensor'):\n",
        "            self.__delattr__('id_tensor')\n",
        "        self.deploy = True\n",
        "\n",
        "\n",
        "\n",
        "class RepVGG(nn.Module):\n",
        "\n",
        "    def __init__(self, num_blocks, num_classes=1000, width_multiplier=None, override_groups_map=None, deploy=False, use_se=False, use_checkpoint=False):\n",
        "        super(RepVGG, self).__init__()\n",
        "        assert len(width_multiplier) == 4\n",
        "        self.deploy = deploy\n",
        "        self.override_groups_map = override_groups_map or dict()\n",
        "        assert 0 not in self.override_groups_map\n",
        "        self.use_se = use_se\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
        "        self.stage0 = RepVGGBlock(in_channels=3, out_channels=self.in_planes, kernel_size=3, stride=2, padding=1, deploy=self.deploy, use_se=self.use_se)\n",
        "        self.cur_layer_idx = 1\n",
        "        self.stage1 = self._make_stage(int(64 * width_multiplier[0]), num_blocks[0], stride=2)\n",
        "        self.stage2 = self._make_stage(int(128 * width_multiplier[1]), num_blocks[1], stride=2)\n",
        "        self.stage3 = self._make_stage(int(256 * width_multiplier[2]), num_blocks[2], stride=2)\n",
        "        self.stage4 = self._make_stage(int(512 * width_multiplier[3]), num_blocks[3], stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.linear = nn.Linear(int(512 * width_multiplier[3]), num_classes)\n",
        "\n",
        "    def _make_stage(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        blocks = []\n",
        "        for stride in strides:\n",
        "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
        "            blocks.append(RepVGGBlock(in_channels=self.in_planes, out_channels=planes, kernel_size=3,\n",
        "                                      stride=stride, padding=1, groups=cur_groups, deploy=self.deploy, use_se=self.use_se))\n",
        "            self.in_planes = planes\n",
        "            self.cur_layer_idx += 1\n",
        "        return nn.ModuleList(blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.stage0(x)\n",
        "        for stage in (self.stage1, self.stage2, self.stage3, self.stage4):\n",
        "            for block in stage:\n",
        "                if self.use_checkpoint:\n",
        "                    out = checkpoint.checkpoint(block, out)\n",
        "                else:\n",
        "                    out = block(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uinJYWQl14yx"
      },
      "source": [
        "# GhostNet\n",
        "<img src=\"https://i.ibb.co/THWKjn7/image.png\" alt=\"image\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE0YNfbDx6_-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtYpy3oyPFTo"
      },
      "source": [
        "# MobileOne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQdy7zLeRGcU"
      },
      "source": [
        "## Memory Cost Access (MCA)\n",
        "- Memoy Cost Access refers to the time and energy required to read from or write to memory in a computing system.\n",
        "- This cost vary depending on several factors: type of memory (e.g, cache, DRAM, or SSD), the location of data (e.g, on-chip or off-chip), and the access pattern (e.g., sequential or random access).\n",
        "- Branching cause increased uintermediate data: each branch typically produces intermediate data that needs to be stored and accessed. This increases the amount of memory used and the frequency of memory accesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYGamStVPHd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7SgOFFgW37qR",
        "79gwLN1MUguo",
        "kwzJ1_PRkGdI",
        "WyHfYtT3UkWQ",
        "DoARvzxSUoUQ",
        "AwOfqf3TWoXB",
        "STflIwmc13PZ"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
