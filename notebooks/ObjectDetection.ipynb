{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Object Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_corner_to_center(boxes):\n",
    "    \"\"\"\n",
    "    boxes: (num_boxes, x1, y1, x2, y2)\n",
    "    where (x1,y1) is the upper corner left coordinate\n",
    "    and   (x2, y2) is the bottom corner right coo\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    boxes = torch.stack((cx, cy, w, h), axis=-1)\n",
    "    return boxes\n",
    "\n",
    "def box_center_to_corner(boxes):\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    boxes = torch.stack((x1, y1, x2, y2), axis=-1)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Anchor Boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateAnchorBoxes:\n",
    "    \"\"\"\n",
    "    Take the image scale size to generate the coordinates of \n",
    "    all anchor boxes given the ratios and sizes\n",
    "    \n",
    "    image: (batch_size, channel, width, height)\n",
    "    sizes: 1D list\n",
    "    ratios: 1D list\n",
    "    \"\"\"\n",
    "    def __init__(self, image, sizes, ratios):\n",
    "        self.image_height, self.image_width = image.shape[-2], image.shape[-1]\n",
    "        self.sizes = torch.tensor(sizes)\n",
    "        self.ratios = torch.tensor(ratios)\n",
    "        self.num_boxes = len(sizes) * len(ratios)\n",
    "        self.create_grid_center_points()\n",
    "        self.create_anchor_width_height()\n",
    "\n",
    "    def create_grid_center_points(self):\n",
    "        self.center_x_axis = (torch.arange(self.image_width) + 0.5) / self.image_width\n",
    "        self.center_y_axis = (torch.arange(self.image_height) + 0.5) / self.image_height\n",
    "        self.grid_x_coords, self.grid_y_coords = torch.meshgrid(self.center_x_axis, self.center_y_axis)\n",
    "        self.grid_x_coords = self.grid_x_coords.reshape(-1)\n",
    "        self.grid_y_coords = self.grid_y_coords.reshape(-1)\n",
    "\n",
    "    def create_anchor_width_height(self):\n",
    "        width_anchor = torch.tensor([])\n",
    "        height_anchor = torch.tensor([])\n",
    "\n",
    "        for size in self.sizes:\n",
    "            for ratio in self.ratios:\n",
    "                width_anchor = torch.cat((width_anchor, size * torch.sqrt(ratio).unsqueeze(0)))\n",
    "                height_anchor = torch.cat((height_anchor, size / torch.sqrt(ratio).unsqueeze(0)))\n",
    "\n",
    "        self.grid_width_height = torch.stack(\n",
    "            (-width_anchor, -height_anchor,\n",
    "            width_anchor, height_anchor)\n",
    "        ).T.repeat(self.image_height * self.image_width, 1) / 2\n",
    "\n",
    "    def mapping(self):\n",
    "        out_grid = torch.stack(\n",
    "            [self.grid_x_coords, self.grid_y_coords,\n",
    "             self.grid_x_coords, self.grid_y_coords],\n",
    "        dim=1).repeat_interleave(self.num_boxes, dim=0)\n",
    "        return out_grid + self.grid_width_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Intersection Over Union**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_box_area(boxes):\n",
    "    return ((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))\n",
    "\n",
    "def BoxIou(boxes1, boxes2):\n",
    "    areas1 = calculate_box_area(boxes1)\n",
    "    areas2 = calculate_box_area(boxes2)\n",
    "\n",
    "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
    "\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(ground_truth, anchors, iou_threshold=0.5):\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    jaccard = BoxIou(anchors, ground_truth)\n",
    "\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long)\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "\n",
    "    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n",
    "    box_j = indices[max_ious >= iou_threshold]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "\n",
    "    return anchors_bbox_map, jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Post Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_boxes(anchors, asigned_bboxes, sigma_xy=0.1, sigma_wh=0.2, mean=0, eps=1e-6):\n",
    "    center_anchors = box_corner_to_center(anchors)\n",
    "    center_asigned_bboxes = box_corner_to_center(asigned_bboxes)\n",
    "    offset_xy = ((center_asigned_bboxes[:, :2] - center_anchors[:, :2]) / center_anchors[:, 2:] - mean) / sigma_xy\n",
    "    offset_wh = (torch.log(eps + center_asigned_bboxes[:, 2:] / center_anchors[:, 2:] - mean)) / sigma_wh\n",
    "    offset = torch.cat([offset_xy, offset_wh], axis=1)\n",
    "    return offset\n",
    "\n",
    "\n",
    "def multibox_target(anchors, labels):\n",
    "\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    num_anchors = anchors.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map, _ = assign_anchor_to_bbox(label[:, 1:], anchors)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(1, 4)\n",
    "\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32)\n",
    "\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "\n",
    "        # Offset transformation\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each group (pixel) has 6 anchor boxes\n"
     ]
    }
   ],
   "source": [
    "# We have 5 scales\n",
    "depths = [64, 128, 128, 128, 128]\n",
    "width =  [32, 16,   8,   4,   1]\n",
    "height = [32, 16,   8,   4,   1]\n",
    "\n",
    "\n",
    "X_multiscale = [\n",
    "    torch.randn(1, depths[0], width[0], height[0]),\n",
    "    torch.randn(1, depths[1], width[1], height[1]),\n",
    "    torch.randn(1, depths[2], width[2], height[2]),\n",
    "    torch.randn(1, depths[3], width[3], height[3]),\n",
    "    torch.randn(1, depths[4], width[4], height[4]),\n",
    "]\n",
    "\n",
    "sizes = [[0.2,  0.272], \n",
    "         [0.37, 0.447], \n",
    "         [0.54, 0.619], \n",
    "         [0.71, 0.79], \n",
    "         [0.88, 0.961]]\n",
    "\n",
    "ratios = [[1, 2, 0.5],\n",
    "          [1, 2, 0.5],\n",
    "          [1, 2, 0.5],\n",
    "          [1, 2, 0.5],\n",
    "          [1, 2, 0.5]]\n",
    "\n",
    "num_anchors_per_pixel = len(sizes[0]) * len(ratios[0])\n",
    "print(f\"Each group (pixel) has {num_anchors_per_pixel} anchor boxes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the anchor boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total anchor boxes are: 6144 -> anchor shape: torch.Size([6144, 4])\n",
      "Total anchor boxes are: 1536 -> anchor shape: torch.Size([1536, 4])\n",
      "Total anchor boxes are: 384 -> anchor shape: torch.Size([384, 4])\n",
      "Total anchor boxes are: 96 -> anchor shape: torch.Size([96, 4])\n",
      "Total anchor boxes are: 6 -> anchor shape: torch.Size([6, 4])\n",
      "torch.Size([8166, 4])\n"
     ]
    }
   ],
   "source": [
    "def prepare_anchor_boxes(X_multiscale, sizes, ratios, num_anchors_per_pixel):\n",
    "    \n",
    "    anchors_multiscale = []\n",
    "\n",
    "    for i in range(len(X_multiscale)):\n",
    "        create_anchor_func = CreateAnchorBoxes(\n",
    "            image=X_multiscale[i], \n",
    "            sizes=sizes[i], \n",
    "            ratios=ratios[i]\n",
    "        )\n",
    "        anchors = create_anchor_func.mapping()\n",
    "        anchors_multiscale.append(anchors)\n",
    "        \n",
    "        total_anchors = X_multiscale[i].shape[2] * X_multiscale[i].shape[3] * num_anchors_per_pixel\n",
    "        print(f\"Total anchor boxes are: {total_anchors} -> anchor shape: {anchors.shape}\")\n",
    "        \n",
    "    anchors_multiscale = torch.cat(anchors_multiscale, dim=0)\n",
    "    return anchors_multiscale\n",
    "        \n",
    "anchors_multiscale = prepare_anchor_boxes(X_multiscale, sizes, ratios, num_anchors_per_pixel)\n",
    "print(anchors_multiscale.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsampling(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            Downsampling(3, 16),\n",
    "            Downsampling(16, 32),\n",
    "            Downsampling(32, 64),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_classes, num_anchors, baseNet=None, avg_pool=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if baseNet is not None:\n",
    "            self.downsample = baseNet\n",
    "        elif avg_pool:\n",
    "            self.downsample = nn.AdaptiveMaxPool2d((1,1))\n",
    "        else:\n",
    "            self.downsample = Downsampling(in_channel, out_channel)\n",
    "            \n",
    "        self.cls_head = nn.Conv2d(out_channel, num_anchors * (num_classes + 1), kernel_size=3, padding=1)\n",
    "        self.bbox_head = nn.Conv2d(out_channel, num_anchors * 4, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.downsample(x)\n",
    "        cls_pred = self.cls_head(x)\n",
    "        bbox_pred = self.bbox_head(x)\n",
    "        return x, cls_pred, bbox_pred\n",
    "    \n",
    "\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self, num_classes, num_anchors):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_preds, self.bbox_preds = [], []\n",
    "        self.base_net = BaseNet()\n",
    "        self.scale_module_1 = ScaleModule(3, 64, num_classes, num_anchors, baseNet=self.base_net)\n",
    "        self.scale_module_2 = ScaleModule(64, 128, num_classes, num_anchors)\n",
    "        self.scale_module_3 = ScaleModule(128, 128, num_classes, num_anchors)\n",
    "        self.scale_module_4 = ScaleModule(128, 128, num_classes, num_anchors)\n",
    "        self.scale_module_5 = ScaleModule(128, 128, num_classes, num_anchors, avg_pool=True)\n",
    "        \n",
    "    def flatten_pred(self, pred):\n",
    "        return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
    "        \n",
    "    def concat_preds(self, preds):\n",
    "        return torch.cat([self.flatten_pred(pred) for pred in preds], dim=1)\n",
    "        \n",
    "    def post_process(self):\n",
    "        self.cls_preds = self.concat_preds(self.cls_preds)\n",
    "        self.bbox_preds = self.concat_preds(self.bbox_preds)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x, cls_pred, bbox_pred = self.scale_module_1(x)\n",
    "        self.cls_preds.append(cls_pred)\n",
    "        self.bbox_preds.append(bbox_pred)\n",
    "        \n",
    "        x, cls_pred, bbox_pred = self.scale_module_2(x)\n",
    "        self.cls_preds.append(cls_pred)\n",
    "        self.bbox_preds.append(bbox_pred)\n",
    "        \n",
    "        x, cls_pred, bbox_pred = self.scale_module_3(x)\n",
    "        self.cls_preds.append(cls_pred)\n",
    "        self.bbox_preds.append(bbox_pred)\n",
    "        \n",
    "        x, cls_pred, bbox_pred = self.scale_module_4(x)\n",
    "        self.cls_preds.append(cls_pred)\n",
    "        self.bbox_preds.append(bbox_pred)\n",
    "        \n",
    "        x, cls_pred, bbox_pred = self.scale_module_5(x)\n",
    "        self.cls_preds.append(cls_pred)\n",
    "        self.bbox_preds.append(bbox_pred)\n",
    "        \n",
    "        self.post_process()\n",
    "        \n",
    "        return x, self.cls_preds, self.bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 3, 256, 256)\n",
    "model = SSD(num_classes=1, num_anchors=6)\n",
    "Y, cls_preds, bbox_preds = model(X)\n",
    "Y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
