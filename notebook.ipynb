{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "\n",
    "\n",
    "class Trim(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :28, :28]\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.LeakyReLU(0.01)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, stride=2, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.LeakyReLU(0.01)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, stride=2, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.LeakyReLU(0.01)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, stride=1, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=2, out_features=3136)\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(in_channels=64, out_channels=64, stride=1, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.LeakyReLU(0.01)\n",
    "        self.trans_conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=64, stride=2, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.LeakyReLU(0.01)\n",
    "        self.trans_conv3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, stride=2, kernel_size=3, padding=0)                \n",
    "        self.relu3 = nn.LeakyReLU(0.01)\n",
    "        self.trans_conv4 = nn.ConvTranspose2d(in_channels=32, out_channels=1, stride=1, kernel_size=3, padding=0)        \n",
    "        self.trim = Trim()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, 64, 7, 7)\n",
    "        x = self.relu1(self.trans_conv1(x))\n",
    "        x = self.relu2(self.trans_conv2(x))\n",
    "        x = self.relu3(self.trans_conv3(x))\n",
    "        x = self.trans_conv4(x)\n",
    "        x = self.trim(x)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.z_mean = torch.nn.Linear(3136, 2)\n",
    "        self.z_log_var = torch.nn.Linear(3136, 2)        \n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def encoding_fn(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        return encoded\n",
    "        \n",
    "    def reparameterize(self, z_mu, z_log_var):\n",
    "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).cuda()\n",
    "        z = z_mu + eps * torch.exp(z_log_var/2.)\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
    "        encoded = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(encoded)\n",
    "        decoded = self.out(decoded)\n",
    "        return encoded, z_mean, z_log_var, decoded\n",
    "    \n",
    "\n",
    "x = torch.randn(1, 1, 28, 28).cuda()\n",
    "model = VAE().cuda()\n",
    "encoded, z_mean, z_log_var, decoded = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Receptive Field with Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from modules.convolution import Conv2d\n",
    "\n",
    "common_weight = None\n",
    "\n",
    "def effective_receptive_field_torch(x: np.ndarray) -> None:\n",
    "    x = torch.tensor(x).to(torch.float32).requires_grad_(True)\n",
    "    conv = nn.Conv2d(in_channels=3, out_channels=4,\n",
    "                     kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "    global common_weight\n",
    "    conv.weight = torch.nn.Parameter(torch.tensor(common_weight))\n",
    "    z = conv(x)\n",
    "      \n",
    "    loss_vec = z[:, :, z.size(-2)//2, z.size(-1)//2]\n",
    "    loss = torch.sum(loss_vec)\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_x =  x.grad[0, 0].detach().numpy()\n",
    "    grad_weight = conv.weight.grad[0, 0].detach().numpy()\n",
    "    print(grad_x.shape)\n",
    "    print(grad_x[112])\n",
    "    print(grad_weight) \n",
    "    \n",
    "\n",
    "def effective_receptive_field_np(x: np.array) -> None:\n",
    "    conv = Conv2d(in_channels=3, out_channels=4, \n",
    "                  padding=1, stride=1, kernel_size=3)\n",
    "    global common_weight\n",
    "    common_weight = conv.weight\n",
    "    z = conv.forward(x)\n",
    "    \n",
    "    grad_z = np.zeros_like(z)\n",
    "    grad_z[:, :, z.shape[-2]//2, z.shape[-1]//2] = 1\n",
    "    \n",
    "    grad_x, grad_weight = conv.backpropagation(np.float32(grad_z))\n",
    "    print(grad_x.shape)\n",
    "    print(grad_x[0, 0, 112])\n",
    "    print(grad_weight[0, 0]) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    image = np.array(Image.open(\"samples/cock.jpg\").resize((224, 224)))\n",
    "    image2 = np.array(Image.open(\"samples/cats.jpg\").resize((224, 224)))\n",
    "    \n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image2 = np.transpose(image2, (2, 0, 1))\n",
    "    batch = np.stack([image, image2], axis=0)\n",
    "\n",
    "    effective_receptive_field_np(batch)\n",
    "    effective_receptive_field_torch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Classifier for Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])  \n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=\"dataset/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"dataset/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=24,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=24,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=128)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.avg(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Model().cuda()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=0.001)\n",
    "epochs = 60\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    step = len(train_loader)\n",
    "    min_loss = 100\n",
    "    for i, (input, target) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        logits = model(input)\n",
    "        loss = loss_func(logits, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    cur_loss = total_loss/step\n",
    "    if cur_loss < min_loss:\n",
    "        min_loss = cur_loss\n",
    "        torch.save(model.state_dict(), \".cache/models/model.pt\")\n",
    "    \n",
    "    print(f\"EPOCH: {epoch+1} ===> Loss: {cur_loss}\") # min loss: 0.22870734333992004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
